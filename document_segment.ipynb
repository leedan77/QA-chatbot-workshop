{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "386484d7-c5f6-4cb0-91d9-15d4cbfa6b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "!pip install tiktoken==0.3.3\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e3ef22-ee2f-4a77-852a-60e35bf59270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a83dd3e-6cca-41a4-9f46-edd58f0d70df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import tiktoken\n",
    "import requests\n",
    "import logging\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04673699-30d1-464e-9893-d13567513833",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding('cl100k_base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81fa92e-ea6d-4ae7-bc5f-2d5626dbbdda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DOC_DIR_PATH = './docs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf601ed-f45b-453c-9205-308461ec520a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba2b9e5d-d510-4c45-b191-6063eecdef6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def num_tokens_from_doc(doc: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of tokens in a text string.\n",
    "    \"\"\"\n",
    "    num_tokens = len(encoding.encode(doc))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eaf1764-5409-4040-89e2-ddf0a3c21c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94711ba2-d2e7-4dec-9a2c-02c895ca8051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def doc_iterator(dir_path: str):\n",
    "    for root, _, filenames in os.walk(dir_path):\n",
    "        for filename in filenames:\n",
    "            file_path = os.path.join(root, filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                with open(file_path, 'r') as file:\n",
    "                    file_contents = file.read()\n",
    "                    yield filename, file_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f56825-da4e-4b1a-8969-765b596d318a",
   "metadata": {},
   "source": [
    "按照token切分文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d65b6825-a183-46ea-a524-31857fcbdd78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd48c3ad-cee4-417a-bba4-fe68f25db3a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_docs = 0\n",
    "n_passages = 0\n",
    "\n",
    "for doc_name, doc in tqdm(doc_iterator(DOC_DIR_PATH)):\n",
    "    print(f\"doc_name: {doc_name}\")\n",
    "    doc_id = doc_name.split('.')[0]\n",
    "    tokens = tokenizer.encode(doc)\n",
    "    chunks = []\n",
    "    chunk_id = 1\n",
    "    n_docs += 1\n",
    "    for i in range(0, len(tokens), CHUNK_SIZE):\n",
    "        chunk_tokens = tokens[i: i+CHUNK_SIZE]\n",
    "        if not len(chunk_tokens) < 256:\n",
    "            chunk = tokenizer.decode(chunk_tokens)\n",
    "            with open(f'./chunks/{doc_id}_{chunk_id}', 'w') as f:\n",
    "                f.write(chunk)\n",
    "            chunk_id += 1\n",
    "            n_passages += 1\n",
    "logger.info(f'{n_docs} documents segmented into {n_passages} passages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a647d8d0-2eb5-4385-b3d2-6934804ac5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "endpoint_name = \"st-paraphrase-mpnet-base-v2-2023-04-14-04-17-29-625-endpoint\"\n",
    "\n",
    "def get_embedding(smr_client, text_input):\n",
    "    parameters = {\n",
    "      #\"early_stopping\": True,\n",
    "      #\"length_penalty\": 2.0,\n",
    "      \"max_new_tokens\": 50,\n",
    "      \"temperature\": 0,\n",
    "      \"min_length\": 10,\n",
    "      \"no_repeat_ngram_size\": 2,\n",
    "    }\n",
    "\n",
    "    response_model = smr_client.invoke_endpoint(\n",
    "                EndpointName=endpoint_name,\n",
    "                Body=json.dumps(\n",
    "                {\n",
    "                    \"inputs\": [text_input],\n",
    "                    \"parameters\": parameters\n",
    "                }\n",
    "                ),\n",
    "                ContentType=\"application/json\",\n",
    "            )\n",
    "    \n",
    "    return response_model['Body'].read().decode('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80113f6-b408-422e-b56c-1a4dd1130053",
   "metadata": {
    "tags": []
   },
   "source": [
    "测试paraphrase-mpnet-base-v2的setence2embedding模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "908bebd0-9667-49e6-96a3-f44aa2326c4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def parseJson2vector(input:str):\n",
    "    \"\"\"\n",
    "    Parse Json string, and extract key \"sentence_embeddings\" to get the vector string, then convert it to numpy array\n",
    "    \"\"\"\n",
    "    json_obj = json.loads(input)\n",
    "    vector_array = json_obj[\"sentence_embeddings\"]\n",
    "    return vector_array\n",
    "\n",
    "def calulate_cosine(vector1,vector2):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two vectors\n",
    "    \"\"\"\n",
    "    return np.dot(vector1,vector2)/(np.linalg.norm(vector1)*np.linalg.norm(vector2))\n",
    "\n",
    "def calulate_cosine_between_sentence_pair(smr_client, q_str, a_str):\n",
    "    q_vec = parseJson2vector(get_embedding(smr_client, q_str))[0]\n",
    "    a_vec = parseJson2vector(get_embedding(smr_client, a_str))[0]\n",
    "    return calulate_cosine(q_vec, a_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e6f764-4cef-46a2-a20b-f110e2886081",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_name, doc in tqdm(doc_iterator(DOC_DIR_PATH)):\n",
    "    if doc_name == \"Cleanroom_FAQ.txt\":\n",
    "        lines = doc.splitlines()\n",
    "        q_lines = [ line for line in lines if line.startswith('Question') ]\n",
    "        a_lines = [ line for line in lines if line.startswith('Answer') ]\n",
    "        for q_idx, q_line in enumerate(q_lines):\n",
    "            max_cos = 0.0\n",
    "            max_a_line = \"\"\n",
    "            for a_idx, a_line in enumerate(a_lines):\n",
    "                cos_val = calulate_cosine_between_sentence_pair(smr_client, q_line, a_line)\n",
    "                if cos_val > max_cos:\n",
    "                    max_cos = cos_val\n",
    "                    max_a_line = a_line\n",
    "            print(f'{max_cos} | {q_line} | {max_a_line}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a709543b-2c99-4750-80f3-ebbeacec07f5",
   "metadata": {},
   "source": [
    "测试bloomz的setence2embedding 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e5162f-9d2f-442e-a00d-de4c434e5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ENDPOINT_NAME='huggingface-textembedding-bloom-7b1-fp1-2023-04-13-11-29-28-700'\n",
    "\n",
    "def get_bloomz_embedding(smr_client, text_input):\n",
    "    payload = {'text_inputs': [text_input]}\n",
    "    payload = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "    response = smr_client.invoke_endpoint(EndpointName=TEXT_EMBEDDING_MODEL_ENDPOINT_NAME, \n",
    "                                                ContentType='application/json', \n",
    "                                                Body=payload)\n",
    "    body = json.loads(response['Body'].read())\n",
    "    embedding = body['embedding'][0]\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07863da6-1172-4a40-abf4-122f22b28046",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bloomz_embedding(smr_client, '请问AWS Clean Rooms的一个协作中可以有多少个参与方?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ec4b3-6cb8-4f8d-87ca-98580d29c945",
   "metadata": {},
   "source": [
    "按照Question和Answer 进行分组测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc513b8-6957-4394-8f91-7f354b321b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calulate_cosine_between_sentence_pair2(smr_client, q_str, a_str):\n",
    "    q_vec = get_bloomz_embedding(smr_client, q_str)\n",
    "    a_vec = get_bloomz_embedding(smr_client, a_str)\n",
    "    return calulate_cosine(q_vec, a_vec)\n",
    "\n",
    "CHUNK_DIR_PATH='./chunks'\n",
    "for doc_name, doc in tqdm(doc_iterator(DOC_DIR_PATH)):\n",
    "    if doc_name == \"Cleanroom_FAQ.txt\":\n",
    "        lines = doc.splitlines()\n",
    "        q_lines = [ line for line in lines if line.startswith('Question') ]\n",
    "        a_lines = [ line for line in lines if line.startswith('Answer') ]\n",
    "        for q_idx, q_line in enumerate(q_lines):\n",
    "            max_cos = 0.0\n",
    "            max_a_doc = \"\"\n",
    "            for doc_name, a_doc in tqdm(doc_iterator(CHUNK_DIR_PATH)):\n",
    "                cos_val = calulate_cosine_between_sentence_pair2(smr_client, q_line, a_doc)\n",
    "                if cos_val > max_cos:\n",
    "                    max_cos = cos_val\n",
    "                    max_a_doc = a_doc\n",
    "            print(f'{max_cos} | {q_line} | {max_a_doc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca5477-d45a-45d1-a0b9-672b2cc5a6b1",
   "metadata": {},
   "source": [
    "按照段落和token_size进行分组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c4cf55d6-c4cb-4966-a59e-13515fd5e374",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len : 162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:52, 56.02s/it] \n"
     ]
    }
   ],
   "source": [
    "paragraphs = []\n",
    "paragraph_embeddings = []\n",
    "q_line_vec_arr = []\n",
    "\n",
    "for doc_name, doc in tqdm(doc_iterator(DOC_DIR_PATH)):\n",
    "    if doc_name == \"Cleanroom_FAQ.txt\":\n",
    "        lines = doc.splitlines()\n",
    "        max_len = len(lines)\n",
    "        print(f\"max_len : {max_len}\")\n",
    "        \n",
    "        q_line_vec_arr = [ (line, parseJson2vector(get_embedding(smr_client, line))[0]) for line in lines if line.startswith('Question') ]\n",
    "        \n",
    "        for line_idx in range(len(lines)):\n",
    "            if lines[line_idx] == '':\n",
    "                continue\n",
    "            span = 0\n",
    "            len_token = 0\n",
    "            while len_token < 128:\n",
    "                # print(f\"line_idx+span : {line_idx+span}, span : {span}\")\n",
    "                delta_token_len = len(tokenizer.encode(lines[line_idx+span]))\n",
    "                span += 1\n",
    "                if line_idx+span == max_len:\n",
    "                    break\n",
    "                len_token += delta_token_len\n",
    "\n",
    "            paragraph = '\\n'.join(lines[line_idx:line_idx+span])\n",
    "            paragraphs.append(paragraph)\n",
    "            paragraph_emb = parseJson2vector(get_embedding(smr_client, paragraph))[0]\n",
    "            # print(paragraph_emb)\n",
    "            paragraph_embeddings.append(paragraph_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "009ece9a-3904-43d7-98aa-896f4325a9ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1db4c256-5e9e-4507-8beb-fe28766db7d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 把这些paragraphs写入文件\n",
    "for idx, paragraph in enumerate(paragraphs):\n",
    "    with open(f'./paragraphs/{idx}.txt', 'w') as f:\n",
    "        f.write(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "76f988cc-5a7d-4088-bfdc-b9682c7a83fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to calulate similiarity\n",
      "***Question: 在中国区是否可用？***\n",
      "Question: 在中国区是否可用？\n",
      "Answer: 目前没有落地中国区的时间表，已经在以下区域推出：美国东部（弗吉尼亚州北部）、美国东部（俄亥俄州）、美国西部（俄勒冈州）、亚太地区（首尔）、亚太地区（新加坡）、亚太地区（悉尼）、亚太地区（东京）、欧洲地区（法兰克福）、欧洲地区（爱尔兰）、欧洲地区（伦敦）和欧洲地区（斯德哥尔摩）\n",
      "[score]:0.7159611368517796\n",
      "-----\n",
      "\n",
      "***Question: 为什么在合成的小数据集上第一次查询的时候需要几分钟才返回？***\n",
      "Question: 为什么在合成的小数据集上第一次查询的时候需要几分钟才返回？\n",
      "Answer: 第一次查询的时候是因为调度和拉起资源的影响，一般第二次查询就会变快。 但过一段时间后，资源释放后这个问题又会出现。在资源拉起期间是不进行收费的。\n",
      "\n",
      "Question: 能支持多大规模数据的查询？查询速度怎么样？\n",
      "Answer: 能支持TB/GB级数据的查询。 一般查询延迟为几十秒到几分钟。默认计算容量为32 CRPUs, 目前这个默认计算容量不可设置，但是roadmap中未来打算让用户可以进行设置。(Slack中Ryan 提到，如果引擎中任务有积压，它能够scale up）\n",
      "[score]:0.766979213775118\n",
      "-----\n",
      "\n",
      "***Question: 能支持多大规模数据的查询？查询速度怎么样？***\n",
      "Answer: 第一次查询的时候是因为调度和拉起资源的影响，一般第二次查询就会变快。 但过一段时间后，资源释放后这个问题又会出现。在资源拉起期间是不进行收费的。\n",
      "\n",
      "Question: 能支持多大规模数据的查询？查询速度怎么样？\n",
      "Answer: 能支持TB/GB级数据的查询。 一般查询延迟为几十秒到几分钟。默认计算容量为32 CRPUs, 目前这个默认计算容量不可设置，但是roadmap中未来打算让用户可以进行设置。(Slack中Ryan 提到，如果引擎中任务有积压，它能够scale up）\n",
      "[score]:0.8072694095261994\n",
      "-----\n",
      "\n",
      "***Question: AWS Clean Rooms 是如何计费的？***\n",
      "Question: AWS Clean Rooms 是如何计费的？\n",
      "Answer: 按照CRPU-hour单价进行计费，每个查询默认计算容量为32 CRPUs。 金额 = (0.125 hours x 32 CRPUs * $0.656 per CRPU-hour) , 有1分钟的最小计费时间。头12个月内，会有9CRPU hours的免费额度。\n",
      "\n",
      "Question: AWS Clean Rooms 从哪里可以看到CRPU-hours的用量？\n",
      "Answer: AWS Clean Rooms 本身的workspace中无法查看，可以在AWS Billing/Bills 中查看Usage Quantity得到该信息。\n",
      "[score]:0.830935465984456\n",
      "-----\n",
      "\n",
      "***Question: AWS Clean Rooms 从哪里可以看到CRPU-hours的用量？***\n",
      "Question: AWS Clean Rooms 从哪里可以看到CRPU-hours的用量？\n",
      "Answer: AWS Clean Rooms 本身的workspace中无法查看，可以在AWS Billing/Bills 中查看Usage Quantity得到该信息。\n",
      "\n",
      "Question: 目前可以支持什么数据源的接入？ \n",
      "Answer: 目前只支持S3，其他数据源近期没有具体计划。\n",
      "\n",
      "Question: 一个协作中，最大的并发查询数是多少？\n",
      "Answer: 5个\n",
      "\n",
      "Question: 我们如何说服客户相信洁净室的安全性和正确性？我在这里的大多数合规计划中都看不到Clean Rooms， https://aws.amazon.com/compliance/services-in-scope/ ？\n",
      "[score]:0.8440821552506292\n",
      "-----\n",
      "\n",
      "***Question: 目前可以支持什么数据源的接入？ ***\n",
      "Answer: 可以这么做，可以提供一些没有任何约束的示例数据给用户（Note: 目前AWS Clean Rooms并没有体现）。\n",
      "\n",
      "Question: 当一个数据贡献者的数据发生更新后会怎么样？\n",
      "Answer: 它是一种live的共享，任何更新会立刻反映到联合分析的结果中。\n",
      "\n",
      "Question: AWS Clean Rooms 未来有哪些前进的方向？\n",
      "Answer: 主要有四个方向：\n",
      "\n",
      "1. Identity matching 身份ID对齐 (Note: 目前这项在官方的PPT在有体现)。\n",
      "[score]:0.6592419323440638\n",
      "-----\n",
      "\n",
      "***Question: 一个协作中，最大的并发查询数是多少？***\n",
      "Answer: 可以，可以参考代码 https://gitlab.aws.dev/rmalecky/aws-clean-rooms-notebooks/-/blob/main/single_collaborator_aggregation.ipynb。\n",
      "\n",
      "Question: 在输出分析结果的时候，能否按照字段进行分区？\n",
      "Answer: 目前不能。\n",
      "\n",
      "Question: 最大的参与方是多少？ 如果超过了限制怎么办？\n",
      "Answer: 目前5个参与方为最大限制，这个是软性的限制，slack频道中有披露最大支持的硬限制为10。\n",
      "[score]:0.6163391677085712\n",
      "-----\n",
      "\n",
      "***Question: 我们如何说服客户相信洁净室的安全性和正确性？我在这里的大多数合规计划中都看不到Clean Rooms， https://aws.amazon.com/compliance/services-in-scope/ ？***\n",
      "Answer: 5个\n",
      "\n",
      "Question: 我们如何说服客户相信洁净室的安全性和正确性？我在这里的大多数合规计划中都看不到Clean Rooms， https://aws.amazon.com/compliance/services-in-scope/ ？\n",
      "Answer: 正在加入这些合规计划的进程中。\n",
      "\n",
      "\n",
      "使用方法与限制相关:\n",
      "\n",
      "Question: 数据源必须在AWS上么？\n",
      "Answer: 对，目前必须在AWS上，而且必须是同一个region。\n",
      "[score]:0.9108781619137524\n",
      "-----\n",
      "\n",
      "***Question: 数据源必须在AWS上么？***\n",
      "Question: 数据源必须在AWS上么？\n",
      "Answer: 对，目前必须在AWS上，而且必须是同一个region。\n",
      "\n",
      "Question: 数据是如何进入到S3？\n",
      "Answer: 需要数据的持有者，把数据上传到S3上，然后再用Glue爬去下，拿到表的schema。这样才能关联到AWS CleanRooms。\n",
      "\n",
      "Question: 这些安全控制权限只是作用与分析么？能够改动它方的数据么？\n",
      "[score]:0.8298499021490512\n",
      "-----\n",
      "\n",
      "***Question: 数据是如何进入到S3？***\n",
      "Question: 数据是如何进入到S3？\n",
      "Answer: 需要数据的持有者，把数据上传到S3上，然后再用Glue爬去下，拿到表的schema。这样才能关联到AWS CleanRooms。\n",
      "\n",
      "Question: 这些安全控制权限只是作用与分析么？能够改动它方的数据么？\n",
      "Answer: 对，只作用于分析，不能修改对方数据\n",
      "\n",
      "Question: 协作方的数据会移动么？协作方的原始数据会集中到Clean rooms 吗？\n",
      "[score]:0.8126777174849155\n",
      "-----\n",
      "\n",
      "***Question: 这些安全控制权限只是作用与分析么？能够改动它方的数据么？***\n",
      "Question: 这些安全控制权限只是作用与分析么？能够改动它方的数据么？\n",
      "Answer: 对，只作用于分析，不能修改对方数据\n",
      "\n",
      "Question: 协作方的数据会移动么？协作方的原始数据会集中到Clean rooms 吗？\n",
      "Answer: 在联合分析获取他方数据时数据存在移动，但协作方的原始数据并不会存住在clean room内，clean room并不是一个物理存贮空间。\n",
      "[score]:0.7601953158533569\n",
      "-----\n",
      "\n",
      "***Question: 协作方的数据会移动么？协作方的原始数据会集中到Clean rooms 吗？***\n",
      "Answer: 对，只作用于分析，不能修改对方数据\n",
      "\n",
      "Question: 协作方的数据会移动么？协作方的原始数据会集中到Clean rooms 吗？\n",
      "Answer: 在联合分析获取他方数据时数据存在移动，但协作方的原始数据并不会存住在clean room内，clean room并不是一个物理存贮空间。\n",
      "\n",
      "Question: 是否发起者和数据贡献者都会被收费？\n",
      "Answer: 是单方收费，只有查询的接收方会进行收费。\n",
      "[score]:0.9220130632980882\n",
      "-----\n",
      "\n",
      "***Question: 是否发起者和数据贡献者都会被收费？***\n",
      "Question: 是否发起者和数据贡献者都会被收费？\n",
      "Answer: 是单方收费，只有查询的接收方会进行收费。\n",
      "\n",
      "Question: 数据贡献方的S3， 会产生API会产生调用次数收费么？\n",
      "Answer: 会， Glue Data Catalog API 的调用也会被收费， 如果加密数据用了KMS-CMK也会被相应的收费。\n",
      "\n",
      "Question: 是否有一个强约束，两方的数据中一定要一个join字段才能够进行分析？\n",
      "[score]:0.7296637996333641\n",
      "-----\n",
      "\n",
      "***Question: 数据贡献方的S3， 会产生API会产生调用次数收费么？***\n",
      "Question: 是否发起者和数据贡献者都会被收费？\n",
      "Answer: 是单方收费，只有查询的接收方会进行收费。\n",
      "\n",
      "Question: 数据贡献方的S3， 会产生API会产生调用次数收费么？\n",
      "Answer: 会， Glue Data Catalog API 的调用也会被收费， 如果加密数据用了KMS-CMK也会被相应的收费。\n",
      "\n",
      "Question: 是否有一个强约束，两方的数据中一定要一个join字段才能够进行分析？\n",
      "[score]:0.8382005365303447\n",
      "-----\n",
      "\n",
      "***Question: 是否有一个强约束，两方的数据中一定要一个join字段才能够进行分析？***\n",
      "Question: 是否有一个强约束，两方的数据中一定要一个join字段才能够进行分析？\n",
      "Answer: List 和Aggregation两种不同的分析规则下有区别， List 只能支持重合用户的，所以必须要有关联字段。Aggregation可以支持你仅仅去查询对方的数据，这种情况下，是可以不指定关联字段的。\n",
      "\n",
      "Question: 如果已经在用Athena，S3桶中已经有数据了，是否能基于S3中这个数据就地加入AWS Clean Room?\n",
      "[score]:0.7504646916076995\n",
      "-----\n",
      "\n",
      "***Question: 如果已经在用Athena，S3桶中已经有数据了，是否能基于S3中这个数据就地加入AWS Clean Room?***\n",
      "Question: 如果已经在用Athena，S3桶中已经有数据了，是否能基于S3中这个数据就地加入AWS Clean Room?\n",
      "Answer: 是的，就地就能加入clean room, 这个S3桶就是一般的S3桶，并没有任何特殊。但这个S3路径不能注册到AWS Lake Formation中。\n",
      "\n",
      "Question: 是否能通过SDK也就是代码来调用Clean room的联合分析\n",
      "Answer: 可以，可以参考代码 https://gitlab.aws.dev/rmalecky/aws-clean-rooms-notebooks/-/blob/main/single_collaborator_aggregation.ipynb。\n",
      "[score]:0.9191357155258938\n",
      "-----\n",
      "\n",
      "***Question: 是否能通过SDK也就是代码来调用Clean room的联合分析***\n",
      "Question: 是否能通过SDK也就是代码来调用Clean room的联合分析\n",
      "Answer: 可以，可以参考代码 https://gitlab.aws.dev/rmalecky/aws-clean-rooms-notebooks/-/blob/main/single_collaborator_aggregation.ipynb。\n",
      "\n",
      "Question: 在输出分析结果的时候，能否按照字段进行分区？\n",
      "Answer: 目前不能。\n",
      "\n",
      "Question: 最大的参与方是多少？ 如果超过了限制怎么办？\n",
      "Answer: 目前5个参与方为最大限制，这个是软性的限制，slack频道中有披露最大支持的硬限制为10。\n",
      "[score]:0.8280682475077911\n",
      "-----\n",
      "\n",
      "***Question: 在输出分析结果的时候，能否按照字段进行分区？***\n",
      "Question: 是否有一个强约束，两方的数据中一定要一个join字段才能够进行分析？\n",
      "Answer: List 和Aggregation两种不同的分析规则下有区别， List 只能支持重合用户的，所以必须要有关联字段。Aggregation可以支持你仅仅去查询对方的数据，这种情况下，是可以不指定关联字段的。\n",
      "\n",
      "Question: 如果已经在用Athena，S3桶中已经有数据了，是否能基于S3中这个数据就地加入AWS Clean Room?\n",
      "[score]:0.5529150987675229\n",
      "-----\n",
      "\n",
      "***Question: 最大的参与方是多少？ 如果超过了限制怎么办？***\n",
      "Answer: 可以，可以参考代码 https://gitlab.aws.dev/rmalecky/aws-clean-rooms-notebooks/-/blob/main/single_collaborator_aggregation.ipynb。\n",
      "\n",
      "Question: 在输出分析结果的时候，能否按照字段进行分区？\n",
      "Answer: 目前不能。\n",
      "\n",
      "Question: 最大的参与方是多少？ 如果超过了限制怎么办？\n",
      "Answer: 目前5个参与方为最大限制，这个是软性的限制，slack频道中有披露最大支持的硬限制为10。\n",
      "[score]:0.594868983577976\n",
      "-----\n",
      "\n",
      "***Question: AWS clean rooms 用的什么加密算法？***\n",
      "Question: AWS clean rooms 用的什么加密算法？\n",
      "Answer: C3R，是aws开源加密代码库。提供了C3R Client(一个可执行的Jar包)，目前仅支持对csv和parquet文件格式进行加密，后续可能会支持更多格式。 由于clean room把所有的字段分成三种类型： 指纹列(fingerprint column), 密封列(sealed column), 明文列(cleartext column), 他们的加密方式有所不同，C3R client 会使用AES-GCM加密算法对sealed字段进行加密，会使用HMAC(Hash-based Message Authentication Code)来对fingerprint字段进行加密。\n",
      "[score]:0.7667938890681019\n",
      "-----\n",
      "\n",
      "***Question: 加密过程中有密文落地么？如果有，密文存在哪里？***\n",
      "Question: 加密过程中有密文落地么？如果有，密文存在哪里？\n",
      "Answer: 由于C3R是客户端加密，所以clean room 关联S3中的数据已经是加密后的密文。\n",
      "\n",
      "Question: 数据上传到S3的过程中，有两种加密方式Server Side 和 Client Side加密，如果客户的安全等级比较高，在数据上传之前做了加密，再想交给clean room 去处理，之前提到的C3R的加密方式，具体是一个怎么样的流程呢？\n",
      "[score]:0.7232662884566999\n",
      "-----\n",
      "\n",
      "***Question: 数据上传到S3的过程中，有两种加密方式Server Side 和 Client Side加密，如果客户的安全等级比较高，在数据上传之前做了加密，再想交给clean room 去处理，之前提到的C3R的加密方式，具体是一个怎么样的流程呢？***\n",
      "Question: 数据上传到S3的过程中，有两种加密方式Server Side 和 Client Side加密，如果客户的安全等级比较高，在数据上传之前做了加密，再想交给clean room 去处理，之前提到的C3R的加密方式，具体是一个怎么样的流程呢？\n",
      "Answer: 首先Clean room 对于Server Side的加密是透明的，无需额外处理。Clean rooms 不支持S3的客户端加密，必须采用C3R客户端进行加密，加密完成以后把数据上传到S3桶，后续流程和不加密的流程是一致的。 加密这步需要比较多的手工操作，包括：\n",
      "[score]:0.9769765664656765\n",
      "-----\n",
      "\n",
      "***Question: AWS Clean Room在安全计算方面，应该归属哪一类？***\n",
      "Question: AWS Clean Rooms 未来有哪些前进的方向？\n",
      "Answer: 主要有四个方向：\n",
      "\n",
      "1. Identity matching 身份ID对齐 (Note: 目前这项在官方的PPT在有体现)。\n",
      "2. 对隐私攻击的防护， 有些查询即使是一些聚合分析，仍然可能探查到个人的信息:\n",
      "    1. 限制访问同一块范围数据的query的数量。\n",
      "[score]:0.7681937952391544\n",
      "-----\n",
      "\n",
      "***Question: 是否所有的字段都可以进行加密？***\n",
      "Question: 是否所有的字段都可以进行加密？\n",
      "Answer: 对于sealed和 fingerprint字段，只有string字段类型被支持。对于csv文件，C3R的客户端处理任何值都作为UTF-8编码的文本，加密前不会做任何其他的前置处理。对于parquet文件，对sealed和 fingerprint字段，如果出现非string的字段，会直接报错。C3R 客户端不能处理parquet中的复杂字段比如struct。\n",
      "\n",
      "Question: 如果要对某个字段进行求和，求平均的数据计算，是否可以加密？\n",
      "[score]:0.7632975999513572\n",
      "-----\n",
      "\n",
      "***Question: 如果要对某个字段进行求和，求平均的数据计算，是否可以加密？***\n",
      "Question: 如果要对某个字段进行求和，求平均的数据计算，是否可以加密？\n",
      "Answer: 不能，只能作为cleartext明文列。\n",
      "\n",
      "Question: 如果想要通过某些字段进行where过滤，这些字段应该是什么类型？\n",
      "Answer: 只能是cleartext明文列。\n",
      "\n",
      "Question: C3R客户端是否有实现任何non-standard的加密算法?\n",
      "Answer: 基本都是标准化的算法，除了一个HKDF(一种密钥推导函数)的实现(来自RFC5869), 但是使用的是java标准加密库中的MAC算法。\n",
      "[score]:0.8166283069476936\n",
      "-----\n",
      "\n",
      "***Question: 如果想要通过某些字段进行where过滤，这些字段应该是什么类型？***\n",
      "Question: 如果想要通过某些字段进行where过滤，这些字段应该是什么类型？\n",
      "Answer: 只能是cleartext明文列。\n",
      "\n",
      "Question: C3R客户端是否有实现任何non-standard的加密算法?\n",
      "Answer: 基本都是标准化的算法，除了一个HKDF(一种密钥推导函数)的实现(来自RFC5869), 但是使用的是java标准加密库中的MAC算法。\n",
      "\n",
      "\n",
      "宏观问题：\n",
      "[score]:0.6562943251109803\n",
      "-----\n",
      "\n",
      "***Question: C3R客户端是否有实现任何non-standard的加密算法?***\n",
      "Answer: C3R，是aws开源加密代码库。提供了C3R Client(一个可执行的Jar包)，目前仅支持对csv和parquet文件格式进行加密，后续可能会支持更多格式。 由于clean room把所有的字段分成三种类型： 指纹列(fingerprint column), 密封列(sealed column), 明文列(cleartext column), 他们的加密方式有所不同，C3R client 会使用AES-GCM加密算法对sealed字段进行加密，会使用HMAC(Hash-based Message Authentication Code)来对fingerprint字段进行加密。\n",
      "[score]:0.7891433280536186\n",
      "-----\n",
      "\n",
      "***Question: AWS Clean rooms 与 AMC的区别与联系是什么？***\n",
      "Question: AWS Clean rooms 与 AMC的区别与联系是什么？\n",
      "Answer: AMC是一个专门服务与Amazon Ads的clean room应用，它是并且将持续是唯一的服务于Amazon Ads客户的应用服务。AWS Clean Rooms 是一个云分析服务，会服务于各个行业的数据合作需求。2023年, AMC 将会把自己的查询引擎和计算基础设置迁移到AWS Clean Rooms服务，将会帮助AMC更方便的服务于客户(他们将不再需要把自己第一方数据上传到AMC，在AWS S3上即可使用)。\n",
      "[score]:0.9005531712288686\n",
      "-----\n",
      "\n",
      "***Question: AWS Clean rooms 与 其他的clean room服务商的区别？***\n",
      "Question: AWS Clean rooms 与 其他的clean room服务商的区别？\n",
      "Answer: AWS Clean rooms 覆盖的客户范围更广。而其他的服务商客户范围相对小，需要把数据移动到他们的平台上。AWS Clean rooms则无需数据移动。(Bastion 2021年announce的时候，其他的云厂商比如(google cloud 和 microsoft azure) 还没有通用场景的clean room solution，snowflake 有，功能上有区别，只允许data provider 提供预先固定的SQL，Bastion灵活性更好。snowflake要求数据必须进入他们的数仓，aws在S3 即可)。\n",
      "[score]:0.841649751545509\n",
      "-----\n",
      "\n",
      "***Question: 有哪些典型的应用场景？***\n",
      "Question: 有哪些典型的应用场景？\n",
      "Answer: 包含多个行业，下面提供部分参考\n",
      "\n",
      "* 广告营销领域：\n",
      "    * 需要进行广告营销活动，流量平台方需要给广告主或者营销方他们的广告点击数据和展现数据进行分析，但是不能提供用户级别的信息。\n",
      "    * 典型客户：营销方或广告主 P&G, Barclays，媒体-流量平台 Amazon Ads, Comcast, NBC Universal\n",
      "[score]:0.7222706570336361\n",
      "-----\n",
      "\n",
      "***Question: AWS Clean Rooms 的 data catalog 是如何实现的？ data sharing permission 是如何实现的？***\n",
      "Question: AWS Clean Rooms 的 data catalog 是如何实现的？ data sharing permission 是如何实现的？\n",
      "Answer: 都是利用了AWS Lake formation, AWS Clean Rooms 里 SQL中字段级别的限制约束，是通过一种new class of AWS Lake formation permission 来实现的。\n",
      "\n",
      "Question: 可以在哪些地方进行Clean room的联合分析？\n",
      "Answer: 可以在clean room 的 workspace， 也可以在Redshift workspace （Note: 从目前发布产品文档上并没有，但是说明背后的引擎就是redshift severless)。\n",
      "[score]:0.9028253856718088\n",
      "-----\n",
      "\n",
      "***Question: 可以在哪些地方进行Clean room的联合分析？***\n",
      "Question: 可以在哪些地方进行Clean room的联合分析？\n",
      "Answer: 可以在clean room 的 workspace， 也可以在Redshift workspace （Note: 从目前发布产品文档上并没有，但是说明背后的引擎就是redshift severless)。\n",
      "\n",
      "Question: 数据提供方如果对联合分析的收益方进行收费，或者实现一个数据授权的合同？\n",
      "Answer: 需要通过AWS Data Exchange 来进行 （Note: 目前AWS Clean Rooms并没有体现）。\n",
      "[score]:0.8501520957296197\n",
      "-----\n",
      "\n",
      "***Question: 数据提供方如果对联合分析的收益方进行收费，或者实现一个数据授权的合同？***\n",
      "Question: 数据提供方如果对联合分析的收益方进行收费，或者实现一个数据授权的合同？\n",
      "Answer: 需要通过AWS Data Exchange 来进行 （Note: 目前AWS Clean Rooms并没有体现）。\n",
      "\n",
      "Question: AWS Clean Rooms 与 AWS Data Exchange 是什么关系？\n",
      "Answer: AWS Clean Rooms 可以通过AWS Data Exchange 去浏览和寻找可用数据的合作方。 他是AWS Data Exchange的更近一步的服务，提供了可控(多种约束限制)和可审计的数据合作方式。\n",
      "[score]:0.7575844850085249\n",
      "-----\n",
      "\n",
      "***Question: AWS Clean Rooms 与 AWS Data Exchange 是什么关系？***\n",
      "Question: AWS Clean Rooms 与 AWS Data Exchange 是什么关系？\n",
      "Answer: AWS Clean Rooms 可以通过AWS Data Exchange 去浏览和寻找可用数据的合作方。 他是AWS Data Exchange的更近一步的服务，提供了可控(多种约束限制)和可审计的数据合作方式。\n",
      "\n",
      "Question: AWS Clean Rooms中是否支持视图？\n",
      "Answer: 允许客户在clean room 创建视图，并且在AWS Clean Rooms中保存物化视图，一旦退出协作，AWS Lake formation permission 将会被撤销，这些物化视图会被删除。（Note: 目前AWS Clean Rooms并没有体现）。\n",
      "[score]:0.9266585824106759\n",
      "-----\n",
      "\n",
      "***Question: AWS Clean Rooms中是否支持视图？***\n",
      "Question: AWS Clean Rooms中是否支持视图？\n",
      "Answer: 允许客户在clean room 创建视图，并且在AWS Clean Rooms中保存物化视图，一旦退出协作，AWS Lake formation permission 将会被撤销，这些物化视图会被删除。（Note: 目前AWS Clean Rooms并没有体现）。\n",
      "\n",
      "Question: 如果数据合作方没有aws account，能否支持？\n",
      "Answer: 目前这个版本不支持，后续的版本可能会考虑（NBC Universal 希望对于没有aws账号另外一方的数据可用）。\n",
      "[score]:0.8384338807073768\n",
      "-----\n",
      "\n",
      "***Question: 如果数据合作方没有aws account，能否支持？***\n",
      "Question: 如果数据合作方没有aws account，能否支持？\n",
      "Answer: 目前这个版本不支持，后续的版本可能会考虑（NBC Universal 希望对于没有aws账号另外一方的数据可用）。\n",
      "\n",
      "Question: 是否能够支持这个协作中，仅仅允许指定运行固定的SQL？\n",
      "Answer: 可以，可以利用query template来做（Note: 目前AWS Clean Rooms并没有体现）。\n",
      "\n",
      "Question: AWS Clean Rooms可以让数据贡献者提供一些样例数据进行预览么？\n",
      "[score]:0.8085108461151757\n",
      "-----\n",
      "\n",
      "***Question: 是否能够支持这个协作中，仅仅允许指定运行固定的SQL？***\n",
      "Question: 是否能够支持这个协作中，仅仅允许指定运行固定的SQL？\n",
      "Answer: 可以，可以利用query template来做（Note: 目前AWS Clean Rooms并没有体现）。\n",
      "\n",
      "Question: AWS Clean Rooms可以让数据贡献者提供一些样例数据进行预览么？\n",
      "Answer: 可以这么做，可以提供一些没有任何约束的示例数据给用户（Note: 目前AWS Clean Rooms并没有体现）。\n",
      "[score]:0.6915471978815693\n",
      "-----\n",
      "\n",
      "***Question: AWS Clean Rooms可以让数据贡献者提供一些样例数据进行预览么？***\n",
      "Question: AWS Clean Rooms可以让数据贡献者提供一些样例数据进行预览么？\n",
      "Answer: 可以这么做，可以提供一些没有任何约束的示例数据给用户（Note: 目前AWS Clean Rooms并没有体现）。\n",
      "\n",
      "Question: 当一个数据贡献者的数据发生更新后会怎么样？\n",
      "Answer: 它是一种live的共享，任何更新会立刻反映到联合分析的结果中。\n",
      "\n",
      "Question: AWS Clean Rooms 未来有哪些前进的方向？\n",
      "[score]:0.9195545306709177\n",
      "-----\n",
      "\n",
      "***Question: 当一个数据贡献者的数据发生更新后会怎么样？***\n",
      "Question: 当一个数据贡献者的数据发生更新后会怎么样？\n",
      "Answer: 它是一种live的共享，任何更新会立刻反映到联合分析的结果中。\n",
      "\n",
      "Question: AWS Clean Rooms 未来有哪些前进的方向？\n",
      "Answer: 主要有四个方向：\n",
      "\n",
      "1. Identity matching 身份ID对齐 (Note: 目前这项在官方的PPT在有体现)。\n",
      "2. 对隐私攻击的防护， 有些查询即使是一些聚合分析，仍然可能探查到个人的信息:\n",
      "[score]:0.7554949511612365\n",
      "-----\n",
      "\n",
      "***Question: AWS Clean Rooms 未来有哪些前进的方向？***\n",
      "Question: AWS Clean Rooms 未来有哪些前进的方向？\n",
      "Answer: 主要有四个方向：\n",
      "\n",
      "1. Identity matching 身份ID对齐 (Note: 目前这项在官方的PPT在有体现)。\n",
      "2. 对隐私攻击的防护， 有些查询即使是一些聚合分析，仍然可能探查到个人的信息:\n",
      "    1. 限制访问同一块范围数据的query的数量。\n",
      "[score]:0.8059676101996932\n",
      "-----\n",
      "\n",
      "***Question: Service Team 有哪些相关的同事？***\n",
      "Question: Service Team 有哪些相关的同事？\n",
      "Answer: Horne, Bill <bgh@amazon.com>, Rababy, Bethany <rababyb@amazon.com>, Malecky, Ryan <rmalecky@amazon.com>, Malik, Mohsen <mmohsen@amazon.com>, Tanna, Shamir <tannas@amazon.com>。\n",
      "[score]:0.732597115948551\n",
      "-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"start to calulate similiarity\")\n",
    "for q_doc, q_vec in q_line_vec_arr:\n",
    "    max_cos = 0.0\n",
    "    a_doc = \"\"\n",
    "    for idx in range(len(paragraphs)):\n",
    "        cos_val = calulate_cosine(q_vec, paragraph_embeddings[idx])\n",
    "        if cos_val > max_cos:\n",
    "            max_cos = cos_val\n",
    "            a_doc = paragraphs[idx]\n",
    "    print(f\"***{q_doc}***\\n{a_doc}\\n[score]:{max_cos}\\n-----\\n\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.12 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.12-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
