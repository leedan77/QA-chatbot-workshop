{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f00e6852-ad5b-43f6-afcc-1bee3d34895e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "devtmpfs         94G     0   94G   0% /dev\n",
      "tmpfs            94G   20K   94G   1% /dev/shm\n",
      "tmpfs            94G  768K   94G   1% /run\n",
      "tmpfs            94G     0   94G   0% /sys/fs/cgroup\n",
      "/dev/nvme0n1p1  160G  144G   17G  90% /\n",
      "/dev/nvme1n1    453G   32G  399G   8% /home/ec2-user/SageMaker\n",
      "tmpfs            19G     0   19G   0% /run/user/1000\n",
      "tmpfs            19G     0   19G   0% /run/user/1002\n",
      "tmpfs            19G     0   19G   0% /run/user/1001\n"
     ]
    }
   ],
   "source": [
    "!df -h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf80354-7edd-4403-9c8b-97a4712f4dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain[all]\n",
    "!pip install sagemaker --upgrade\n",
    "!pip install  boto3\n",
    "!pip install requests_aws4auth\n",
    "!pip install requests\n",
    "!pip install transformer\n",
    "!pip install opensearch-py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b216f11-a2b5-4f0b-b4de-060a1a64076a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## initial sagemaker env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "19f05414-d44b-464e-a0c7-46c317378ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::687912291502:role/webui-notebook-stack-ExecutionRole-62U5FV4LJQS\n",
      "sagemaker bucket: sagemaker-us-west-2-687912291502\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "from typing import Dict\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3aed78-e14b-4d30-93d3-2871d6603bd8",
   "metadata": {},
   "source": [
    "## intial lanchain lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4aae9f94-a8df-49b5-b22b-f89e8d756b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import LLMChain\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"]= \"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\"\n",
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "hostname=\"vpc-llm-rag-aos-seg3mzhpp76ncpxezdqtcsoiga.us-west-2.es.amazonaws.com\"\n",
    "region='us-west-2'\n",
    "username=\"admin\"\n",
    "passwd=\"(OL>0p;/\"\n",
    "index_name=\"qa_index\"\n",
    "size=10\n",
    "\n",
    "class EmbeddingContentHandler(ContentHandlerBase):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": prompt, **model_kwargs})\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"vectors\"]\n",
    "\n",
    "embedding_content_handler = EmbeddingContentHandler()\n",
    "sm_embeddings = SagemakerEndpointEmbeddings(\n",
    "    # endpoint_name=\"endpoint-name\", \n",
    "    # credentials_profile_name=\"credentials-profile-name\", \n",
    "    #endpoint_name=\"huggingface-textembedding-bloom-7b1-fp1-2023-04-17-03-31-12-148\", \n",
    "    endpoint_name=\"st-paraphrase-mpnet-base-v2-2023-04-17-10-05-10-718-endpoint\",\n",
    "    region_name=\"us-west-2\", \n",
    "    content_handler=embedding_content_handler\n",
    ")\n",
    "\n",
    "\n",
    "class ExtractContentHandler(ContentHandlerBase):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"ask\": prompt})\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        model_predictions = json.loads(output.read().decode(\"utf-8\"))\n",
    "        generated_text = model_predictions[\"answer\"]\n",
    "        return generated_text\n",
    "\n",
    "feature_extraction_handler = ExtractContentHandler()\n",
    "feature_extraction_llm=SagemakerEndpoint(\n",
    "        endpoint_name=\"pytorch-inference-2023-04-20-07-28-31-042\", \n",
    "        region_name=\"us-west-2\", \n",
    "        content_handler=feature_extraction_handler\n",
    ")\n",
    "\n",
    "class TextGenContentHandler(ContentHandlerBase):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({prompt: prompt, **model_kwargs})\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[0][\"generated_text\"]\n",
    "\n",
    "text_gen_content_handler = TextGenContentHandler()\n",
    "sm_llm=SagemakerEndpoint(\n",
    "        endpoint_name=\"bloomz-7b1-mt-2023-04-17-02-49-58-645-endpoint\", \n",
    "        region_name=\"us-west-2\", \n",
    "        model_kwargs={\"temperature\":1e-10},\n",
    "        content_handler=text_gen_content_handler\n",
    ")\n",
    "\n",
    "opensearch_vector_search = OpenSearchVectorSearch(\n",
    "    \"https://\"+hostname+\":443\",\n",
    "    index_name,\n",
    "    sm_embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd74903-b3ba-4ed3-a019-d1acfbb2c054",
   "metadata": {},
   "source": [
    "## func(for local test ) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "b0819cc2-5be1-4f78-8826-90c3ff2c72d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import os\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import LLMChain\n",
    "\n",
    "source_includes = [\"question\",\"answer\"]\n",
    "runtime= boto3.client('runtime.sagemaker')\n",
    "headers = { \"Content-Type\": \"application/json\" }\n",
    "\n",
    "endpoint_name=\"pytorch-inference-2023-04-20-07-28-31-042\"\n",
    "region_name=\"us-west-2\"\n",
    "\n",
    "class ExtractContentHandler(ContentHandlerBase):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"ask\": prompt})\n",
    "        return input_str.encode('utf-8')\n",
    "\n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        model_predictions = json.loads(output.read().decode(\"utf-8\"))\n",
    "        generated_text = model_predictions[\"answer\"]\n",
    "        return generated_text\n",
    "feature_extraction_handler = ExtractContentHandler()\n",
    "feature_extraction_llm=SagemakerEndpoint(\n",
    "    endpoint_name=endpoint_name,\n",
    "    region_name=region_name,\n",
    "    content_handler=feature_extraction_handler\n",
    ")\n",
    "\n",
    "########parse k-NN search resule###############\n",
    "# input:\n",
    "#  r: AOS returned json\n",
    "# return:\n",
    "#  result : array of topN text\n",
    "#############################################\n",
    "def parse_results(r):\n",
    "    res = []\n",
    "    result = []\n",
    "    print(r)\n",
    "    for i in range(len(r['hits']['hits'])):\n",
    "        h = r['hits']['hits'][i]\n",
    "        if h['_source']['question'] not in clean:\n",
    "            result.append(h['_source']['question'])\n",
    "            res.append('<第'+str(i+1)+'条信息>'+h['_source']['question'] + '。</第'+str(i+1)+'条信息>\\n')\n",
    "    print(res)\n",
    "    return result\n",
    "\n",
    "########get embedding vector by SM llm########\n",
    "# input:\n",
    "#  questions:question texts(list)\n",
    "#  sm_client: sagemaker runtime client\n",
    "#  sm_endpoint:Sagemaker embedding llm endpoint\n",
    "# return:\n",
    "#  result : vector of embeded text\n",
    "#############################################\n",
    "def get_vector_by_sm_endpoint(questions,sm_client,endpoint_name,parameters):\n",
    "    response_model = sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\": questions,\n",
    "                \"parameters\": parameters\n",
    "            }\n",
    "        ),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    json_str = response_model['Body'].read().decode('utf8')\n",
    "    json_obj = json.loads(json_str)\n",
    "    embeddings = json_obj['sentence_embeddings']\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "########get embedding vector by lanchain vector search########\n",
    "# input:\n",
    "#  questions:question texts(list0\n",
    "#  embedings:lanchain embeding models\n",
    "# return:\n",
    "#  result : vector of embeded text\n",
    "#############################################################\n",
    "def get_vector_by_lanchain(questions , embedings):\n",
    "    doc_results = embeddings.embed_documents(questions)\n",
    "    print(doc_results[0])\n",
    "    return doc_results\n",
    "\n",
    "\n",
    "########k-nn search by lanchain########\n",
    "# input:\n",
    "#  q:question text\n",
    "#  vectorSearch: lanchain VectorSearch instance\n",
    "# return:\n",
    "#  result : k-NN search result\n",
    "#############################################################\n",
    "def search_using_lanchain(question, vectorSearch):\n",
    "    docs = vectorSearch.similarity_search(query)\n",
    "    return docs\n",
    "\n",
    "\n",
    "\n",
    "########k-nn by native AOS########\n",
    "# input:\n",
    "#  q_embedding:embeded question text(array)\n",
    "#  index:AOS k-NN index name\n",
    "#  hostname: aos endpoint\n",
    "#  username: aos username\n",
    "#  passwd: aos password\n",
    "#  source_includes: fields to return\n",
    "#  k: topN\n",
    "# return:\n",
    "#  result : k-NN search result\n",
    "#############################################################\n",
    "def search_using_aos_knn(q_embedding, hostname, username,passwd, index, source_includes, size):\n",
    "    awsauth = (username, passwd)\n",
    "    print(type(q_embedding))\n",
    "    query = {\n",
    "        \"size\": size,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"sentence_vector\": {\n",
    "                    \"vector\": q_embedding,\n",
    "                    \"k\": size\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    r = requests.post(\"https://\"+hostname +\"/\"+ index + '/_search', auth=awsauth, headers=headers, json=query)\n",
    "    return r.text\n",
    "\n",
    "\n",
    "\n",
    "########k-nn ingestion by native AOS########\n",
    "# input:\n",
    "#  docs:ingestion source documents\n",
    "#  index:AOS k-NN index name\n",
    "#  hostname: aos endpoint\n",
    "#  username: aos username\n",
    "#  passwd: aos password\n",
    "# return:\n",
    "#  result : N/A\n",
    "#############################################################\n",
    "def k_nn_ingestion_by_aos(docs,index,hostname,username,passwd):\n",
    "    auth = (username, passwd)\n",
    "    search = OpenSearch(\n",
    "        hosts = [{'host': hostname, 'port': 443}],\n",
    "        ##http_auth = awsauth ,\n",
    "        http_auth = auth ,\n",
    "        use_ssl = True,\n",
    "        verify_certs = True,\n",
    "        connection_class = RequestsHttpConnection\n",
    "    )\n",
    "    for doc in docs:\n",
    "        vector_field = doc['sentence_vector']\n",
    "        question_filed = doc['question']\n",
    "        answer_field = doc['answer']\n",
    "        document = { \"question\": question_filed, 'answer':answer_field, \"sentence_vector\": vector_field}\n",
    "        search.index(index=index, body=document)\n",
    "\n",
    "\n",
    "########k-nn ingestion by lanchain #########################\n",
    "# input:\n",
    "#  docs:ingestion source documents\n",
    "#  vectorStore: lanchain AOS vectorStore instance\n",
    "# return:\n",
    "#  result : N/A\n",
    "#############################################################\n",
    "def k_nn_ingestion_by_lanchain(docs,vectorStore):\n",
    "    for doc in docs:\n",
    "        opensearch_vector_search.add_texts(docs,batch_size=10)\n",
    "\n",
    "\n",
    "########topic extraction by lanchain sm endpoint #########################\n",
    "# input:\n",
    "#  docs: history QA list[(Q1,A1).(Q2,A2),(Q3,A3)]\n",
    "#  k: output extracted words limit\n",
    "#  llm: lanchain llm instance\n",
    "# return:\n",
    "#  result : output extracted features\n",
    "#############################################################\n",
    "def feature_extraction_by_lanchain(docs,k,sm_endpoint_nm,sm_region):\n",
    "    #feature_extraction_handler = ExtractContentHandler()\n",
    "    #feature_extraction_llm=SagemakerEndpoint(\n",
    "    #    endpoint_name=sm_endpoint_nm,\n",
    "    #    region_name=sm_region,\n",
    "    #    content_handler=feature_extraction_handler\n",
    "    #)\n",
    "    global feature_extraction_llm\n",
    "    if feature_extraction_llm is None:\n",
    "        feature_extraction_llm=SagemakerEndpoint(\n",
    "            endpoint_name=sm_endpoint_nm,\n",
    "            region_name=sm_region,\n",
    "            content_handler=feature_extraction_handler\n",
    "        )\n",
    "    payload = \"\"\n",
    "    for doc in docs:\n",
    "        question, answer = doc\n",
    "        payload=payload+\"Q:\"+question+\"\\n\"\n",
    "        payload=payload+\"A:\"+answer+\"\\n\"\n",
    "    payload = payload+\"问题：主题摘要,限制在\"+str(k)+\"个字内\"\n",
    "    extracted_feature_texts=feature_extraction_llm(payload)\n",
    "    return extracted_feature_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa302e0a-b8ed-4fa0-934e-ab4ff0740316",
   "metadata": {
    "tags": []
   },
   "source": [
    "## major chain pipeline ################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ef73a-d691-4d30-b35e-1a5254cdb8d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1: data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "9bb9c564-9d44-4ba3-a279-34b6df2aa3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_question = \"\"\"在中国区是否可用？\n",
    "为什么在合成的小数据集上第一次查询的时候需要几分钟才返回？\n",
    "能支持多大规模数据的查询？查询速度怎么样？\n",
    "AWS Clean Rooms 是如何计费的？\n",
    "AWS Clean Rooms 从哪里可以看到CRPU-hours的用量？\n",
    "目前可以支持什么数据源的接入？ \n",
    "一个协作中，最大的并发查询数是多少？\n",
    "我们如何说服客户相信洁净室的安全性和正确性？我在这里的大多数合规计划中都看不到Clean Rooms， https://aws.amazon.com/compliance/services-in-scope/ ？\n",
    "数据源必须在AWS上么？\n",
    "数据是如何进入到S3？\n",
    "这些安全控制权限只是作用与分析么？能够改动它方的数据么？\n",
    "协作方的数据会移动么？协作方的原始数据会集中到Clean rooms 吗？\n",
    "是否发起者和数据贡献者都会被收费？\n",
    "数据贡献方的S3， 会产生API会产生调用次数收费么？\n",
    "是否有一个强约束，两方的数据中一定要一个join字段才能够进行分析？\n",
    "如果已经在用Athena，S3桶中已经有数据了，是否能基于S3中这个数据就地加入AWS Clean Room?\n",
    "是否能通过SDK也就是代码来调用Clean room的联合分析\n",
    "在输出分析结果的时候，能否按照字段进行分区？\n",
    "最大的参与方是多少？ 如果超过了限制怎么办？\n",
    "AWS clean rooms 用的什么加密算法？\n",
    "加密过程中有密文落地么？如果有，密文存在哪里？\n",
    "数据上传到S3的过程中，有两种加密方式Server Side 和 Client Side加密，如果客户的安全等级比较高，在数据上传之前做了加密，再想交给clean room 去处理，之前提到的C3R的加密方式，具体是一个怎么样的流程呢？\n",
    "AWS Clean Room在安全计算方面，应该归属哪一类？\n",
    "是否所有的字段都可以进行加密？\n",
    "如果要对某个字段进行求和，求平均的数据计算，是否可以加密？\n",
    "如果想要通过某些字段进行where过滤，这些字段应该是什么类型？\n",
    "C3R客户端是否有实现任何non-standard的加密算法?\n",
    "AWS Clean rooms 与 AMC的区别与联系是什么？\n",
    "AWS Clean rooms 与 其他的clean room服务商的区别？\n",
    "有哪些典型的应用场景？\n",
    "AWS Clean Rooms 的 data catalog 是如何实现的？ data sharing permission 是如何实现的？\n",
    "可以在哪些地方进行Clean room的联合分析？\n",
    "数据提供方如果对联合分析的收益方进行收费，或者实现一个数据授权的合同？\n",
    "AWS Clean Rooms 与 AWS Data Exchange 是什么关系？\n",
    "AWS Clean Rooms中是否支持视图？\n",
    "如果数据合作方没有aws account，能否支持？\n",
    "是否能够支持这个协作中，仅仅允许指定运行固定的SQL？\n",
    "AWS Clean Rooms可以让数据贡献者提供一些样例数据进行预览么？\n",
    "当一个数据贡献者的数据发生更新后会怎么样？\n",
    "AWS Clean Rooms 未来有哪些前进的方向？\n",
    "Service Team 有哪些相关的同事\n",
    "\"\"\"\n",
    "questions = all_question.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "565ebb18-ac48-4e27-8e86-a49cd221398f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_answer = \"\"\"目前没有落地中国区的时间表，已经在以下区域推出：美国东部（弗吉尼亚州北部）、美国东部（俄亥俄州）、美国西部（俄勒冈州）、亚太地区（首尔）、亚太地区（新加坡）、亚太地区（悉尼）、亚太地区（东京）、欧洲地区（法兰克福）、欧洲地区（爱尔兰）、欧洲地区（伦敦）和欧洲地区（斯德哥尔摩）\n",
    "第一次查询的时候是因为调度和拉起资源的影响，一般第二次查询就会变快。 但过一段时间后，资源释放后这个问题又会出现。在资源拉起期间是不进行收费的。\n",
    "能支持TB/GB级数据的查询。 一般查询延迟为几十秒到几分钟。默认计算容量为32 CRPUs, 目前这个默认计算容量不可设置，但是roadmap中未来打算让用户可以进行设置。(Slack中Ryan 提到，如果引擎中任务有积压，它能够scale up）\n",
    "按照CRPU-hour单价进行计费，每个查询默认计算容量为32 CRPUs。 金额 = (0.125 hours x 32 CRPUs * $0.656 per CRPU-hour) , 有1分钟的最小计费时间。头12个月内，会有9CRPU hours的免费额度。\n",
    "AWS Clean Rooms 本身的workspace中无法查看，可以在AWS Billing/Bills 中查看Usage Quantity得到该信息。\n",
    "目前只支持S3，其他数据源近期没有具体计划。\n",
    "5个\n",
    "正在加入这些合规计划的进程中。\n",
    "对，目前必须在AWS上，而且必须是同一个region。\n",
    "需要数据的持有者，把数据上传到S3上，然后再用Glue爬去下，拿到表的schema。这样才能关联到AWS CleanRooms。\n",
    "对，只作用于分析，不能修改对方数据\n",
    "在联合分析获取他方数据时数据存在移动，但协作方的原始数据并不会存住在clean room内，clean room并不是一个物理存贮空间。\n",
    "是单方收费，只有查询的接收方会进行收费。\n",
    "会， Glue Data Catalog API 的调用也会被收费， 如果加密数据用了KMS-CMK也会被相应的收费。\n",
    "List 和Aggregation两种不同的分析规则下有区别， List 只能支持重合用户的，所以必须要有关联字段。Aggregation可以支持你仅仅去查询对方的数据，这种情况下，是可以不指定关联字段的。\n",
    "是的，就地就能加入clean room, 这个S3桶就是一般的S3桶，并没有任何特殊。但这个S3路径不能注册到AWS Lake Formation中。\n",
    "可以，可以参考代码 https://gitlab.aws.dev/rmalecky/aws-clean-rooms-notebooks/-/blob/main/single_collaborator_aggregation.ipynb\n",
    "目前不能\n",
    "目前5个参与方为最大限制，这个是软性的限制，slack频道中有披露最大支持的硬限制为10.\n",
    "C3R，是aws开源加密代码库。提供了C3R Client(一个可执行的Jar包)，目前仅支持对csv和parquet文件格式进行加密，后续可能会支持更多格式。 由于clean room把所有的字段分成三种类型： 指纹列(fingerprint column), 密封列(sealed column), 明文列(cleartext column), 他们的加密方式有所不同，C3R client 会使用AES-GCM加密算法对sealed字段进行加密，会使用HMAC(Hash-based Message Authentication Code)来对fingerprint字段进行加密。\n",
    "由于C3R是客户端加密，所以clean room 关联S3中的数据已经是加密后的密文。\n",
    "首先Clean room 对于Server Side的加密是透明的，无需额外处理。Clean rooms 不支持S3的客户端加密，必须采用C3R客户端进行加密，加密完成以后把数据上传到S3桶，后续流程和不加密的流程是一致的。 加密这步需要比较多的手工操作，包括：* 加密前需要创建好collaboration(协作), 得到collaboration_id后续在加密中需要提供 * 利用openssl 生成32位密钥并分享给其他协作方。* 加密过程中需要指定哪些字段为指纹列(fingerprint column), 密封列(sealed column), 明文列(cleartext column)\n",
    "(待补充)\n",
    "对于sealed和 fingerprint字段，只有string字段类型被支持。对于csv文件，C3R的客户端处理任何值都作为UTF-8编码的文本，加密前不会做任何其他的前置处理。对于parquet文件，对sealed和 fingerprint字段，如果出现非string的字段，会直接报错。C3R 客户端不能处理parquet中的复杂字段比如struct。\n",
    "不能，只能作为cleartext明文列\n",
    "基本都是标准化的算法，除了一个HKDF(一种密钥推导函数)的实现(来自RFC5869), 但是使用的是java标准加密库中的MAC算法。\n",
    "AMC是一个专门服务与Amazon Ads的clean room应用，它是并且将持续是唯一的服务于Amazon Ads客户的应用服务。AWS Clean Rooms 是一个云分析服务，会服务于各个行业的数据合作需求。2023年, AMC 将会把自己的查询引擎和计算基础设置迁移到AWS Clean Rooms服务，将会帮助AMC更方便的服务于客户(他们将不再需要把自己第一方数据上传到AMC，在AWS S3上即可使用).\n",
    "AWS Clean rooms 覆盖的客户范围更广。而其他的服务商客户范围相对小，需要把数据移动到他们的平台上。AWS Clean rooms则无需数据移动。(Bastion 2021年announce的时候，其他的云厂商比如(google cloud 和 microsoft azure) 还没有通用场景的clean room solution，snowflake 有，功能上有区别，只允许data provider 提供预先固定的SQL，Bastion灵活性更好。snowflake要求数据必须进入他们的数仓，aws在S3 即可)。\n",
    "\"\"\"\n",
    "answers=all_answer.split(\"\\n\")\n",
    "\n",
    "answer_2 = \"\"\"\n",
    "包含多个行业，下面提供部分参考\n",
    "* 广告营销领域：\n",
    "    * 需要进行广告营销活动，流量平台方需要给广告主或者营销方他们的广告点击数据和展现数据进行分析，但是不能提供用户级别的信息。\n",
    "    * 典型客户：营销方或广告主 P&G, Barclays，媒体-流量平台 Amazon Ads, Comcast, NBC Universal\n",
    "* 零售领域\n",
    "    * 银行和零售商需要获取他们的重叠用户，用于进行联合的市场活动，但不要把客户的其他信息暴露给彼此。（比如非场合用户）\n",
    "* 医疗健康领域 (结合之前的HCLS Slides， 并不是来自Bastion)\n",
    "    * 药厂和医院之间，药厂需要医院的病历数据； 药厂和外包的研发机构之间需要进行数据的共享。\n",
    "    * 典型客户：Change Healthcare, AstraZeneca \n",
    "* 其他领域的一些典型的客户\n",
    "    * 数据服务商 Foursquare ，Nielsen， IRI， \n",
    "    * 其他 Cars.com\n",
    "\"\"\"\n",
    "answers.append(answer_2)\n",
    "    \n",
    "all_answer3=\"\"\"都是利用了AWS Lake formation, AWS Clean Rooms 里 SQL中字段级别的限制约束，是通过一种new class of AWS Lake formation permission 来实现的。\n",
    "可以在clean room 的 workspace， 也可以在Redshift workspace （Note: 从目前发布产品文档上并没有，但是说明背后的引擎就是redshift severless)\n",
    "需要通过AWS Data Exchange 来进行 （Note: 目前AWS Clean Rooms并没有体现）\n",
    "AWS Clean Rooms 可以通过AWS Data Exchange 去浏览和寻找可用数据的合作方。 他是AWS Data Exchange的更近一步的服务，提供了可控(多种约束限制)和可审计的数据合作方式\n",
    "允许客户在clean room 创建视图，并且在AWS Clean Rooms中保存物化视图，一旦退出协作，AWS Lake formation permission 将会被撤销，这些物化视图会被删除。（Note: 目前AWS Clean Rooms并没有体现）\n",
    "目前这个版本不支持，后续的版本可能会考虑（NBC Universal 希望对于没有aws账号另外一方的数据可用）\n",
    "可以，可以利用query template来做（Note: 目前AWS Clean Rooms并没有体现）\n",
    "可以这么做，可以提供一些没有任何约束的示例数据给用户（Note: 目前AWS Clean Rooms并没有体现）\n",
    "它是一种live的共享，任何更新会立刻反映到联合分析的结果中\n",
    "\"\"\"\n",
    "answers=answers+(all_answer3.split(\"\\n\"))\n",
    "\n",
    "answer_4=\"\"\"\n",
    "主要有四个方向：\n",
    "1. Identity matching 身份ID对齐 (Note: 目前这项在官方的PPT在有体现)\n",
    "2. 对隐私攻击的防护， 有些查询即使是一些聚合分析，仍然可能探查到个人的信息\n",
    "    1. 限制访问同一块范围数据的query的数量\n",
    "    2. 采用差分隐私(Differential Privacy) 结果中添加噪声(会影响分析的精度)，Morgen Stanley 作为这个功能的beta用户\n",
    "3. 机器学习，P&G 表现出这方面的需求， 应该也是基于表格数据的模型，可能是流失预测，人群聚类等场景\n",
    "4. 和DSP的集成，直接把激活用户ID给到对接的DSP，而不通过cleanroom的数据接收方\n",
    "\"\"\"\n",
    "answers.append(answer_4)\n",
    "               \n",
    "answer_5=\"\"\"Horne, Bill <bgh@amazon.com>, Rababy, Bethany <rababyb@amazon.com>, Malecky, Ryan <rmalecky@amazon.com>, Malik, Mohsen <mmohsen@amazon.com>, Tanna, Shamir <tannas@amazon.com>\"\"\"\n",
    "answers.append(answer_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8099c8-53e1-4581-9e87-381f5cf90020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"./code/\")\n",
    "#import func\n",
    "\n",
    "\n",
    "parameters = {\n",
    "      #\"early_stopping\": True,\n",
    "      #\"length_penalty\": 2.0,\n",
    "      \"max_new_tokens\": 50,\n",
    "      \"temperature\": 0,\n",
    "      \"min_length\": 10,\n",
    "      \"no_repeat_ngram_size\": 2,\n",
    "}\n",
    "endpoint_name=\"st-paraphrase-mpnet-base-v2-2023-04-17-10-05-10-718-endpoint\"\n",
    "##########embedding by llm model##############\n",
    "#sentense_vectors=get_vector_by_sm_endpoint(questions[0:5],sm_client,endpoint_name,parameters)\n",
    "sentense_vectors=get_vector_by_sm_endpoint(questions[5:10],sm_client,endpoint_name,parameters)\n",
    "#sentense_vectors = get_vector_by_lanchain(questions , sm_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86909a45-83bf-4c7e-a1b9-6b3eba6ca726",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(sentense_vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "793a20d5-fdd0-4be2-835d-8afc753062a1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ConnectionTimeout",
     "evalue": "ConnectionTimeout caused by - ConnectTimeout(HTTPSConnectionPool(host='vpc-llm-rag-aos-seg3mzhpp76ncpxezdqtcsoiga.us-west-2.es.amazonaws.com', port=443): Max retries exceeded with url: /qa_index/_doc (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f639ba2eca0>, 'Connection to vpc-llm-rag-aos-seg3mzhpp76ncpxezdqtcsoiga.us-west-2.es.amazonaws.com timed out. (connect timeout=10)')))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             conn = connection.create_connection(\n\u001b[0m\u001b[1;32m    175\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mtimeout\u001b[0m: timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    704\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSocketTimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             raise ConnectTimeoutError(\n\u001b[0m\u001b[1;32m    180\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPSConnection object at 0x7f639ba2eca0>, 'Connection to vpc-llm-rag-aos-seg3mzhpp76ncpxezdqtcsoiga.us-west-2.es.amazonaws.com timed out. (connect timeout=10)')",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    490\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             retries = retries.increment(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='vpc-llm-rag-aos-seg3mzhpp76ncpxezdqtcsoiga.us-west-2.es.amazonaws.com', port=443): Max retries exceeded with url: /qa_index/_doc (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f639ba2eca0>, 'Connection to vpc-llm-rag-aos-seg3mzhpp76ncpxezdqtcsoiga.us-west-2.es.amazonaws.com timed out. (connect timeout=10)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectTimeout\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/opensearchpy/connection/http_requests.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepared_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    552\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNewConnectionError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mConnectTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectTimeout\u001b[0m: HTTPSConnectionPool(host='vpc-llm-rag-aos-seg3mzhpp76ncpxezdqtcsoiga.us-west-2.es.amazonaws.com', port=443): Max retries exceeded with url: /qa_index/_doc (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f639ba2eca0>, 'Connection to vpc-llm-rag-aos-seg3mzhpp76ncpxezdqtcsoiga.us-west-2.es.amazonaws.com timed out. (connect timeout=10)'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionTimeout\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_42799/2606303178.py\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#########ingestion into aos ###################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mk_nn_ingestion_by_aos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0musername\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpasswd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;31m#k-nn_ingestion_by_lanchain(docs,opensearch_vector_search)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_42799/987417916.py\u001b[0m in \u001b[0;36mk_nn_ingestion_by_aos\u001b[0;34m(docs, index, hostname, username, passwd)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0manswer_field\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mquestion_filed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'answer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0manswer_field\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sentence_vector\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvector_field\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0msearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/opensearchpy/client/utils.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/opensearchpy/client/__init__.py\u001b[0m in \u001b[0;36mindex\u001b[0;34m(self, index, body, id, params, headers)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mdoc_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"_doc\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         return self.transport.perform_request(\n\u001b[0m\u001b[1;32m    356\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSKIP_IN_PATH\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"PUT\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0m_make_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/opensearchpy/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/opensearchpy/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                 status, headers_response, data = connection.perform_request(\n\u001b[0m\u001b[1;32m    371\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p39/lib/python3.9/site-packages/opensearchpy/connection/http_requests.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"N/A\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mConnectionTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TIMEOUT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"N/A\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionTimeout\u001b[0m: ConnectionTimeout caused by - ConnectTimeout(HTTPSConnectionPool(host='vpc-llm-rag-aos-seg3mzhpp76ncpxezdqtcsoiga.us-west-2.es.amazonaws.com', port=443): Max retries exceeded with url: /qa_index/_doc (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f639ba2eca0>, 'Connection to vpc-llm-rag-aos-seg3mzhpp76ncpxezdqtcsoiga.us-west-2.es.amazonaws.com timed out. (connect timeout=10)')))"
     ]
    }
   ],
   "source": [
    "docs=[]\n",
    "for index, sentence_vector in enumerate(sentense_vectors):\n",
    "    #print(index, sentence_vector)\n",
    "    doc = {\n",
    "        \"question\":questions[index],\n",
    "        \"answer\": answers[index],\n",
    "        \"sentence_vector\": sentence_vector\n",
    "          }\n",
    "    docs.append(doc)\n",
    "#data = [{str(i): j for i, j in zip(['question', 'answer','sentence_vector'], values)} for values in zip(questions, answers,sentense_vectors)]\n",
    "#docs = json.dumps(data)\n",
    "#print(len(docs[2]['sentence_vector']))\n",
    "\n",
    "#########ingestion into aos ###################\n",
    "k_nn_ingestion_by_aos(docs,index_name,hostname,username,passwd)\n",
    "#k-nn_ingestion_by_lanchain(docs,opensearch_vector_search)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f6fad1-bd2e-4070-8696-2c904d30910e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2:KNN search topN questio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff4f8d6-1ece-40b0-a7bb-12cce30ddb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########2:KNN search topN question#########################\n",
    "query = \"\"\"what's the functions of cleaning room?\"\"\"\n",
    "query_embedding = get_vector_by_sm_endpoint(query,sm_client,endpoint_name,parameters)\n",
    "context = parse_results(search_using_aos_knn(query_embedding, hostname, username,passwd, \n",
    "                                           index, source_includes, size))\n",
    "#context = parse_results(search_using_lanchain(question, vectorSearch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d17056-7e66-49d7-9e02-9e4869b92841",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3:warp up context ，construct prompt and call llm lanchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358b36bb-54d5-48f3-a43d-f6e075f2f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########3:warp up context ，construct prompt and call llm lanchain######3\n",
    "prompt_template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=5)\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=sm_llm,\n",
    "    prompt=PROMPT,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "chain({\"input_documents\": NONE, \"question\": query, \"context\": context}, return_only_outputs=True)\n",
    "memory.load_memory_variables({})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c3468f30-bb00-4d7f-a786-d2492dc66da2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'大语言模型 BLOOM 的优化方法包括GPU 融合、cosine 学习率调度、使用 warmup 和降温策略。'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload1 = \"\"\"question:GPU优化作用？\n",
    "              answer:一般来说，GPU无法在检索数据同时执行这些计算。此外，现代GPU的计算性能远远高于每个操作(被称为GPU编程中的核)所需的内存传输速度。\n",
    "核融合是一种基于GPU计算的优化方法，通过在一次内核调用中执行多个连续操作。该方法提供了一种最小化数据传输的方法：中间结果留在GPU寄存器中，而不是复制到VRAM，从而节省开销。\n",
    "我们使用了Megatron-LM提供了几个定制化融合CUDA核。首先，我们使用一个优化核来执行LayerNorm，以及用核来融合各种缩放、掩码和softmax操作的各种组合。\n",
    "使用Pytorch的JIT功能将一个偏差项添加至GeLU激活中。作为一个使用融合核的例子，在GeLU操作中添加偏差项不会增加额外的时间，因为该操作受内存限制：与GPU VRAM和寄存器之间的数据传输相比，额外的计算可以忽略不计。\n",
    "因此融合这两个操作基本上减少了它们的运行时间\n",
    "              question：如何优化BLOOM？\n",
    "              answer：我们使用上表3中详细描述的超参数来训练BLOOM的6个尺寸变体。\n",
    "架构和超参数来自于我们的实验结果(Le Scao et al.)和先前的训练大语言模型(Brown et al.)。\n",
    "非176B模型的深度和宽度大致遵循先前的文献(Brown et al.)，偏离的3B和7.1B只是为了更容易适合我们训练设置。\n",
    "由于更大的多语言词表，BLOOM的embedding参数尺寸更大。在开发104B参数模型的过程中，我们使用了不同的Adam \n",
    "参数、权重衰减和梯度裁剪来对目标稳定性进行实验，但没有发现其有帮助。\n",
    "对于所有模型，我们在410B tokens使用cosine学习率衰减调度，在计算允许的情况下，将其作为训练长度的上限，并对375M tokens进行warmup。\n",
    "我们使用权重衰减、梯度裁剪，不使用dropout。ROOTS数据集包含341B tokens的文本。然而，基于训练期间发布的修订scaling laws，我们决定在重复数据上对大模型进行额外25B tokens的训练。\n",
    "由于warmup tokens + decay tokens大于总的token数量，所以学习率衰减始终未达到终点\n",
    "            问题：主题摘要,限制在20个字内\n",
    "           \"\"\"\n",
    "feature_extraction_llm(payload1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b9f98e3a-9ba3-45ca-b358-519bdc187646",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'优化GPU使用和BLOOM模型训练，包括使用核融合和定制化CUDA核，减少数据传输和时间，使用Adam学习率调度和权重衰减，避免稳定性问题。在重复数据上增加额外训练25B tokens，但学习率衰减未达到终点。'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=[(\"GPU优化作用\",\"\"\"一般来说，GPU无法在检索数据同时执行这些计算。此外，现代GPU的计算性能远远高于每个操作(被称为GPU编程中的核)所需的内存传输速度。\n",
    "核融合是一种基于GPU计算的优化方法，通过在一次内核调用中执行多个连续操作。该方法提供了一种最小化数据传输的方法：中间结果留在GPU寄存器中，而不是复制到VRAM，从而节省开销。\n",
    "我们使用了Megatron-LM提供了几个定制化融合CUDA核。首先，我们使用一个优化核来执行LayerNorm，以及用核来融合各种缩放、掩码和softmax操作的各种组合。\n",
    "使用Pytorch的JIT功能将一个偏差项添加至GeLU激活中。作为一个使用融合核的例子，在GeLU操作中添加偏差项不会增加额外的时间，因为该操作受内存限制：与GPU VRAM和寄存器之间的数据传输相比，额外的计算可以忽略不计。\n",
    "因此融合这两个操作基本上减少了它们的运行时间\"\"\"),\n",
    "      (\"如何优化BLOOM\",\"\"\"我们使用上表3中详细描述的超参数来训练BLOOM的6个尺寸变体。\n",
    "架构和超参数来自于我们的实验结果(Le Scao et al.)和先前的训练大语言模型(Brown et al.)。\n",
    "非176B模型的深度和宽度大致遵循先前的文献(Brown et al.)，偏离的3B和7.1B只是为了更容易适合我们训练设置。\n",
    "由于更大的多语言词表，BLOOM的embedding参数尺寸更大。在开发104B参数模型的过程中，我们使用了不同的Adam \n",
    "参数、权重衰减和梯度裁剪来对目标稳定性进行实验，但没有发现其有帮助。\n",
    "对于所有模型，我们在410B tokens使用cosine学习率衰减调度，在计算允许的情况下，将其作为训练长度的上限，并对375M tokens进行warmup。\n",
    "我们使用权重衰减、梯度裁剪，不使用dropout。ROOTS数据集包含341B tokens的文本。然而，基于训练期间发布的修订scaling laws，我们决定在重复数据上对大模型进行额外25B tokens的训练。\n",
    "由于warmup tokens + decay tokens大于总的token数量，所以学习率衰减始终未达到终点\"\"\")]\n",
    "feature_extraction_by_lanchain(docs,40,\"pytorch-inference-2023-04-20-07-28-31-042\",\"us-west-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f3ccfac-ab97-48ad-ae79-c6465b3fe16e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OpenSearch([{'host': 'vpc-llm-rag-aos-seg3mzhpp76ncpxezdqtcsoiga.us-west-2.es.amazonaws.com', 'port': 443}])>\n"
     ]
    }
   ],
   "source": [
    "auth = ('admin', '(OL>0p;/')\n",
    "search = OpenSearch(\n",
    "         hosts = [{'host': hostname, 'port': 443}],\n",
    "         ##http_auth = awsauth ,\n",
    "         http_auth = auth ,\n",
    "         use_ssl = True,\n",
    "         verify_certs = True,\n",
    "         connection_class = RequestsHttpConnection\n",
    "     )\n",
    "print(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c8d869-76e3-44e7-89c4-946983d6349a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
