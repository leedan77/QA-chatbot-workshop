Question: 为什么EMR使用S3作为数据存储能带来性能优势？
Answer: 
使用S3作为存储层的优势：

* 使用S3存储首先可以节省存储成本。HDFS使用底层EBS卷作为存储层。通过对比S3与EBS卷的成本，EBS卷的存储成本约为S3的4倍。如果HDFS做了3副本，则成本会是S3的12倍。
* 可以实现存储无限扩展，同时存储与计算解耦。
* 对于访问频率低的数据，可以通过S3智能分层的方式，移动到更便宜的S3存储层（例如IA，Glacier），进一步减少成本
* 集群终止后，HDFS数据会丢失，S3基本不会有丢失数据风险

需要注意：

* 即使集群使用S3作为持久性数据存储，HDFS仍会运行在集群内。例如临时文件、jar包的存储与分发仍会用到HDFS。
* HDFS使用本地块存储，相比S3的对象存储，访问HDFS一般来说会更快一些。若是应用对性能有更高的需求，可以再考虑使用HDFS

Question: EMR使用S3做为数据存贮，S3文件处理有什么要注意的？
Answer: 在处理写入S3上的文件时，考虑3个点：
* 压缩：压缩文件可以减少存储成本，以及S3与EMR节点之间的网络传输。在压缩时需要注意使用可切分的压缩格式
* 合并小文件：避免小文件。一般小于128MB的单个文件视为小文件。通过合并小文件，可以减少S3的LIST请求数，并提升job的性能。
* 使用列式存储格式：列式存储格式如Parquet与ORC可以提升查询性能。在仅查询部分字段的情况下，列式存储格式可以提供更好的性能

Question: EMR的job运行时应该怎么选择计算硬件?
Answer: 
大部分EMR集群可以运行在M系这类常规负载类型的实例，例如m5.xlarge，m6g.xlarge等。
如果集群的负载大部分为计算密集型任务，则可以考虑计算优化型的实例类型，例如C5系列。
如果有较多的内存缓存需求，例如spark常规任务，则可以考虑使用内存优化型的内存实例，例如R5系列。
 
对于Master节点来说，一般没有大量计算需求。对于大部分小于50个节点的集群，可以考虑使用常规类型，例如m5系列。不过，由于Master节点会运行关键服务（例如Resource Manager，Namenode，Hiveserver2等），所以一般建议使用较大的实例类型（例如8xlarge以上）。除此之外，单Master集群会有单点故障问题。对关键业务，建议使用Multi-Master配置。

Question: EMR集群何时应该选择带instance store的实例类型
Answer: 如果EMR集群需要承载高I/O负载，则可以考虑选择带instance store类型的实例。例如I3en或d3en，这类实例专门用于data-intensive负载。它们自带的NVMe SSD实例存储，可以以低成本的方式提供低延迟、高随机 I/O性能，高顺序读吞吐以及高IOPS的能力。例如在重度HDFS使用，或是有大量spark shuffle数据读写的场景，可以以更低的成本达到更高的性能。
 
Question: EMR中使用Graviton2实例能有什么样的性能提升？
Answer: 在EMR 6.1.0与5.31.0之后的版本，支持了EC2 graviton实例类型。这种实例类型相较x86架构实例（例如m5系），可以提供更高的性价比，也就是更高的性能的同时，成本更低。
基于新的额Graviton2实例，相较于上一代实例类型，EMR runtime for Spark可以提供额外最高30%成本节省，并提升15%的性能。
更多有关此部分信息，可以参考文档：
https://aws.amazon.com/cn/blogs/big-data/amazon-emr-now-provides-up-to-30-lower-cost-and-up-to-15-improved-performance-for-spark-workloads-on-graviton2-based-instances/
 
Question: EMR对于不同负载场景，应该怎么选择合适的价格模型？
Answer: 对于有些场景，有持续的最小计算资源的需求，例如Spark streaming，ad-hoc集群等。可以使用预留实例或是saving plan节省成本。

更多这部分信息可以参考
https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-instance-purchasing-options.html

Question: 如何采用Spot实例来降低EMR的成本？
Answer: 
Spot实例可以提高高达90%的折扣（对比On-Demand价格），所以尽可能的可以使用spot实例降低成本。
 
在EC2回收Spot实例时，会有2分钟的警告。而由于EMR上运行的应用如spark，mr，hive等原生支持了task重试的功能，所以在task由于spot节点回收而导致失败的情况下，会自动在其他节点重试task。对整个job的正常运行基本不会有太大影响。
 
EMR 使用spot实例最佳实践：
https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-spot-instances
 
Question: EMR Managed Scaling功能介绍
Answer: 
在EMR 5.30.0以及之后版本（除了6.0.0），可以使用EMR managed scaling。此功能可以自动的根据集群负载扩展并收缩节点资源。配置也非常简单。
 
相较于EMR原始的auto scaling功能，managed scaling的灵敏度更高，反应更快。且策略为托管策略，不需要用户手动再去配置扩缩容策略。而是使用EMR内置的最佳实践策略进行自动扩缩容。
 
Managed scaling的介绍文档：
https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-managed-scaling.html
 
Question: Application Container的size有什么讲究？
Answer:
默认情况下，EMR在集群启动时，会自动根据节点的大小设置YARN与Spark的内存资源。此时，便需要了解各个节点类型的资源量，以避免集群资源使用不充分。
 
在EMR层面，会自动根据节点类型设置yarn-site.xml的yarn.nodemanager.resource.cpu-vcores 与 yarn.nodemanager.resource.memory-mb参数，来决定有多少资源可以提供给yarn。例如，对于m5.4xlarge节点类型，其本身提供了16 vcore与64GB的内存。但是在EMR集群里，其能提供给yarn的资源为16 vcore与57344 MB。此时，便需要考虑请求的资源量是否能充分利用到集群的资源量。
 
例如，假设设置：
* spark.executor.memory: 20,000M
* spark.yarn.executor.memoryOverhead: 10% (2,000M)
* spark.executor.cores: 4
 
Spark仅能在每个m5.4xlarge运行两个executor，并导致节点有57,344-44,000 (22,000*2) = 13,344MB的内存资源无法分配使用。节点资源利用率仅有76.6%。在节点数量多的情况下，便会看到YARN queue里资源还很充足，但是job仍有排队的现象。
 
如果我们根据节点的yarn.nodemanager.resource.memory-mb配置调整spark.executor.memory的配置，则可以达到更高的资源利用率。例如：

* spark.executor.memory: 12,000M 
* spark.yarn.executor.memoryOverhead: 10% (1,200M) 
* spark.executor.cores: 4

此时每个节点可以分配4个executor，此时仅有4,544 MB的内存资源未分配，内存资源使用率可达到92%。

更多有关Spark与YARN调整size的说明，可以参考文档：
https://aws.amazon.com/cn/blogs/big-data/best-practices-for-successfully-managing-memory-for-apache-spark-applications-on-amazon-emr/
 
每个节点类型提供EMR Yarn的资源量可以参考文档：
https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-task-config.html
 
Question: 启用Ganglia监控集群资源是怎样帮助降低EMR集群的成本的？
Answer: 
在创建集群时，建议启动Ganglia对集群资源使用实际情况进行监控。对资源使用情况有了进一步了解后，才有成本节省的空间。
 
需要注意：Yarn web UI只能看到yarn层面分配资源的情况，无法真正了解底层资源使用情况。所以需要EMR提供的Ganglia进行监控
 
EMR的监控有3种模式，分别为CloudWatch，Ganglia，以及第三方监控系统（例如Grafana与Prometheus）。
 
不管是哪种监控方式，建议的监控指标包括：

* CPU使用率
* 内存使用率
* 磁盘使用率

这3个指标可以帮助我们更好的评估集群的负载。例如，假设看到Ganglia里显示集群CPU或内存使用率达到了100%，但是其他资源使用率不高。则可以考虑更换实例类型，提供更高的性能或减少集群节点。例如：

* 如果集群节点为R4或M5系列机型，此时在Ganglia内看到CPU使用率达到100%，但是内存使用率低于50%。则可以考虑使用C4系列节点类型，因为集群性能瓶颈在CPU
* 如果CPU与内存使用率均在50%，则可以减少集群大小，缩减成本
 
需要注意：这部分的建议更适用于固定负载的集群或是临时集群。如果集群是长时间运行或负载不可预估，则建议使用managed scaling来实现集群资源的自动扩缩容。
 
有关EMR Cloudwatch指标可以参考文档：
https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_ViewingMetrics.html
 
有关EMR集成Grafana与Prometheus方案，可以参考文档：
https://aws.amazon.com/cn/blogs/big-data/monitor-and-optimize-analytic-workloads-on-amazon-emr-with-prometheus-and-grafana/
 
Question: 不同业务负载拆分集群是如何降低EMR的成本的？
Answer: 
EMR集群根据工作负载，可以考虑拆分不同的集群。例如对于Flink实时计算集群，其资源使用量基本固定，且一般为常驻集群，这部分可以单独使用一个集群。并为集群节点购买RI或saving plan，节省long-running集群的成本。
 
对于batch ETL的集群，可以考虑临时集群。在需要完成batch ETL工作时，通过pipeline的方式，拉起一个新集群执行ETL。ETL作业的提交方式可以通过EMR step实现。在EMR step完成后，可以自动终止集群，节省成本。对此集群可以结合使用on-demand与spot的实例，并根据SLA需求调整两者比例。
 
Question: EMR的可靠性最佳实践是什么？
Answer:
1. 将所有集群视为瞬态集群
将EMR视为瞬态集群有以下几个优点：
* 让更新，打补丁，AMI更新，以及基础设施的变更更简单
* 快速从故障恢复
* 减少运维long-running集群的成本
* 可以实现仅在有job时运行，提升性价比

在实现EMR瞬态集群时，需要考虑的点：
* 将集群的配置、启动以自动化脚本的方式完成
* 集群为无状态集群，数据存储使用S3，元数据管理使用外部元数据管理（例如rds mysql，aws glue datacatalog）
* 与外部服务解耦

2. Core节点使用On-Demand，Task使用Spot
Core节点运行了HDFS，spot节点强制回收的机制会导致节点上的数据丢失。
Task节点仅执行task（运行 yarn nodemanager进程），节点丢失会导致task失败。Task的重试会在计算框架层保障（例如spark/mapreduce task重试）。
 
3. 使用Instance fleet，申请不同类型的实例类型
Instance fleet可以组合多种实例类型，达到指定的计算资源。

4. 处理S3 503 slow down问题
在EMR on S3的架构下，随着业务量增加，对S3的请求数也会增加。此时可能会出现S3 Slow Down的问题。
默认S3的请求quota为（到S3的prefix级别）：
* PUT/COPY/POST/DELETE请求：3500个/s
* GET/HEAD请求：5500个/s

一般来说，有几种方式处理S3 503 slow down问题：
 
4.1. 使用EMRFS重试
EMRFS是EMR内的工具包，用于对S3进行读写（例如spark task在访问S3时，在EMR内默认使用EMRFS）。可以理解为类似开源s3a的工具包，但是由aws内部实现。经过测试，EMRFS性能比开源版本s3a性能更好。
 
EMRFS提供了2种方式提升S3请求的成功率。可以在 emrfs-site.xml里进行配置。

* 在遇到S3 slow down或者请求受限等问题时，默认会使用指数回退的方式进行重试。最高重试次数默认为4。可以修改emrfs-site.xml文件内的fs.s3.maxRetires参数为更高，提高重试次数（例如20-50）
* 启用additive-increase/multiplicative-decrease（AIMD）的重试逻辑。AIMD在EMR 6.4.0及以上版本后支持

更多信息可参考：
https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-emrfs-retry.html

4.2. 增加fs.s3n.multipart.uploads.split.size大小
此参数表示的是：在做s3分段上传时，分段的最大size，单位为bytes。默认为134217728（134mb），最高可以设置为5368709120 (5GB)。
设置更大的值可以减少对S3的请求。在设置时可以先尝试一些中间值（例如 1-2gb），观察是否对性能有所影响。
 
更多信息可以参考：
https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-upload-s3.html#Config_Multipart

4.3. 减少对S3的请求
可以尝试在应用层减少对S3的每秒请求数，例如：
* 如果slow down发生在写阶段，则可以减少write task的并行度。例如Spark可以使用coalesce() 或 repartition() 的方法减少Spark输出的partition数，然后再写S3。或者减少每个executor的vcore数也可以达到减少对s3请求的目的
* slow down发生在读阶段，则可以在源数据那段做小文件合并，减少S3读文件的请求
* job分散到不同的小时或天来执行。减少同一时段多个job同时访问S3的情况

关于这部分更多信息，可以参考：
https://aws.amazon.com/cn/premiumsupport/knowledge-center/emr-s3-503-slow-down/
 
4.4. 优化S3的数据分布
S3的Rate limit（3500写和5500读）是应用在prefix级别的。可以根据访问业务的模式，来调整S3的分区，减少S3 slow down的问题。
例如下面两种prefix：
s3://<bucket1>/dt=2021-11-01

s3://<bucket2>/product=1/dt=2021-11-01
s3://<bucket2>/product=2/dt=2021-11-01
 
第二种prefix的方式可以达到比第一种prefix方式更高的请求频率。
第一种方式在dt=2021-11-01只能达到默认的3500/s 写请求以及5500/s的读请求。而第二种方式可以达到7000/s写请求以及11000/s读请求。
S3上良好的分区分布，除了可以增加请求的并发量外，也是一种有效过滤数据的手段（分区裁剪），同样也可以减少发往S3的请求数。

4.5. 审计并更新EMR与EC2的limit
AWS对各个不同的服务基本都有资源与API的limit限制。对于EMR的限制，主要有2种：
* 资源限制：例如EC2资源。EMR请求的EC2资源同样遵循账户limit的限制，若是有弹性扩缩以及多集群的需求，可以预先了解当前账户EC2的limit，并及时做limit提升；
* 限制：主要是对EMR服务发起的API请求。例如DescribeStep API，DescribeCluster API等。如果有这方面的需求，则可以考虑在发起API端做指数回退，以及分散API请求（例如避免集中在某个时间点同时发起大量API请求）

更多这部分信息可以参考以下文档：
https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-service-limits-what-are.html
 
5. 避免HDFS单副本
EMR内hdfs的dfs.replication的配置与CORE节点数有关：
* CORE节点数 < 4，hdfs副本数为1
* CORE节点数 < 10，hdfs副本数为2
* 其他情况，hdfs副本数为3

节点数小于4时，hdfs副本默认为1。对于SPOT Core节点，或是长时间运行的集群，会有丢失数据风险。在这种情况下，建议启动的集群至少包含2个core节点，并调整dfs.replication参数为2。
此外，hdfs副本为1时，如果有hbase的regionserver下线，会影响到WAL的拆分，并导致HMaster启动异常。

另外需要注意的点：

* 不要让core节点数少于dfs.replication。例如，如果dfs.replication=3，则保持core节点至少为3台
* 增加hdfs副本数需要额外的EBS存储空间
* 在缩减CORE节点时，也会考虑到副本因子。EMR不允许节点缩小个数到比副本因子还小。例如假设hdfs副本因子为2，CORE节点数为2，则EMR不会缩减CORE节点为1（即使在控制台手动缩减）

有关EMR默认HDFS副本数请参考文档：
https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hdfs-config.html

6. 监控磁盘使用量，避免UNHEALTHY NODES问题
当core节点或task节点的磁盘使用率（主要是/mnt，/mnt1等数据卷）超过90%时（由yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage参数指定），磁盘会标注为unhealthy。如果节点healthy磁盘比率小于25%，则NodeManager会将此节点标注为unhealthy，并告知ResourceManager。之后便会不再向此节点分配containers。
节点保持unhealthy状态超过45分钟后，如果集群终止保护未开启，则ResourceManager会对节点做decommission的操作。如果集群终止保护已开启，则仅会对task节点做decommission操作。Core节点仍保持为UNHEALTHY状态。
导致磁盘使用率超过90%的2个常见原因是：HDFS数据以及spark/hive shuffle数据。为了避免这种情况，建议预先根据使用情况配置EBS卷大小。或者也可以增加更多的EBS卷，或者对现有EBS卷进行扩容，防止卷使用率超过90%。

在监控与报警层面，有几点可以尝试：
* 通过CloudWatch监控HDFS使用率。有助于判断磁盘使用率超过90%是否与HDFS用量相关
* 通过部署监控脚本（例如CloudWatch监控脚本）监控节点的磁盘使用率
* 在集群监控方面，可以通过对集群的MRUnhealthyNodes指标设置CloudWatch警报，在发现有unhealthy node时提前报警

更多有关此部分信息，可以参考文档：
https://aws.amazon.com/cn/premiumsupport/knowledge-center/emr-exit-status-100-lost-node/
https://docs.aws.amazon.com/emr/latest/ManagementGuide/UsingEMR_TerminationProtection.html
 
计算集群所需的HDFS空间大小：
https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-instances-guidelines.html#emr-plan-instances-hdfs

7. 使用外部代理节点连接集群

尽量不要登录到Master节点做脚本编写的工作，并将脚本与代码保存在Master节点。若是集群异常终止，则会导致Master节点上所有数据丢失。
 
同时也不建议直接连接到Master节点，因为Master运行了如Resource Manager，Namenode，HiveServer2等关键进程。防止误操作导致服务异常。可以在外部构建一个代理节点做与集群交互的操作，例如job提交，收集集群信息等。
 
代理节点的配置部分可以参考以下文档：
https://laurence.blog.csdn.net/article/details/108529087
https://github.com/bluishglc/emr-edgenode-maker
 
Question: 如何处理EMR的时区问题？
Answer: EMR节点默认时区为UTC，若是有需要调整所有节点时区的需求，则建议在BA脚本里加上以下内容：
#!/bin/bash
sudo  echo "Zone=Asia/Shanghai" > /etc/sysconfig/clock
sudo  ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime

Question: bootstrap脚本问题的作用是？
Answer:
在集群规划阶段，一定要用上bootstrap脚本。即使当前没有bootstrap脚本需求，也可以使用一个空脚本将这个位置占上。否则一旦集群启动后，则不可再配置添加bootstrap脚本。
 
Bootstrap脚本以hadoop用户运行，可以使用sudo执行root命令。脚本是在安装勾选的hadoop等应用前运行，所以无法用脚本直接修改这些应用配置。如果需要添加jar到这些应用的lib目录，则可以分为2个脚本：

* 脚本A的作用是下载脚本B，然后在后台运行脚本B。这样可以让BA阶段结束，进入安装框架阶段
* 脚本B的在后台检查目标框架是否已安装，例如目标lib目录已存在。如果存在则将jar包从s3下载并放到目标lib目录；否则继续等待

BA脚本也可以选择性地在master还是slave节点运行，参考文档：
https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-plan-bootstrap.html
 
Question: ApplicationMaster启动位置问题
Answer:
在EMR 5.x版本，使用了node label功能。并将application master 仅调度在CORE节点上执行。此原因之一是由于：TASK节点一般为spot节点，App Master在spot节点上有随着spot节点回收而失败的风险。
在EMR 6.x版本后，YARN node label功能默认已disabled。App Master默认可以运行在core与task节点上。若是需要启用node label功能，则需要开启以下配置：
yarn.node-labels.enabled:  true
yarn.node-labels.am.default-node-label-expression:  'CORE'

CORE节点只启动App Master
在5.x EMR下，App Master仅启动在CORE节点上，若是CORE资源被普通task资源占用，则会导致可供给App Master的资源减少，从而导致可能的application没有资源而排队的情况。
继而便有客户提出需求：希望CORE节点只运行App Master。经过测试，步骤如下：
a.  关闭  Resource Manager. 
b.  注释掉 capacity-scheduler.xml 中所有 CORE 相关的条目。
c.  启动  Resource Manager. 
d.  使用以下命令将 CORE label 改为 exclusive. 
#  yarn rmadmin -removeFromClusterNodeLabels CORE 
#  yarn rmadmin -addToClusterNodeLabels "CORE(exclusive=true)" 
#  yarn cluster —list-node-labels 
Node  Labels: <CORE:exclusivity=true>
e. 去掉步骤b 里 capacity-scheduler.xml 中的注释
f. 重启 Resource Manager

Question: 自动扩缩容问题
Answer:
当前EMR Managed Scaling扩缩容主要依赖的指标为Yarn相关指标进行扩缩容。例如Spark，Hive，Mapreduce等都可以根据资源的需求在EMR内进行自动的计算资源扩缩容。
 
对于并非基于Yarn资源管理的组件，例如HBase，Presto，Trino等。EMR Managed Scaling无法对这些资源进行扩缩容。此时需要部署额外的脚本，收集应用指标，并打到CloudWatch监控。再使用EMR 的 autoscaling功能，手动指定扩缩容的规则，才可实现对这些应用的自动扩缩。
 
 有关下线 Core 节点时的影响，从测试结果来看：

1. HDFS单副本时，下线Core节点，会对HBase有影响；如果为2副本，则可以正常下线Core节点，HBase未受到影响
2. 在测试的EMR 6.5 与6.8版本中，无论是单主还是多主集群，通过reconfiguration修改HDFS的副本数，仅会重启HDFS相关进程（NN与DN）。其他如RS，NM，RM均未重启

Question: EBS卷扩展问题
Answer:
在节点EBS卷使用率过高时，可以对EBS卷进行扩展。其扩展步骤与EC2的EBS卷扩展步骤一致。
EBS卷resizing文档：
https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/requesting-ebs-volume-modifications.html
需要注意，在控制台对EBS卷做了扩容操作后，还需要登录到EC2实例，对其做分区扩展与文件系统扩展的操作。
中文操作系统注意，在做分区扩展时，中文locale会导致磁盘扩容命令出错，所以需要用LC_ALL=C强制指定用英文的locale：

$ LC_ALL=C sudo growpart /dev/nvme1n1 1  
CHANGED: partition=1 start=2048 old: size=1048573952 end=1048576000 new: size=1258289119 end=1258291167

Question: Yarn分配资源问题
Answer:
EMR内默认使用的YARN资源调度器为DefaultResourceCalculator。这种资源调度器的工作模式仅考虑内存资源，并不考虑vcpu资源，所以仅根据申请的内存资源来分配container。
此时便会在yarn web UI界面看到vcpu资源使用与预期申请不匹配的情况。
Yarn另一种资源调度器为DominantResourceCalculator，它会同时考虑到集群可用的vcpu与memory资源，再分配container。
 
Question: Spark Thrift Server问题
Answer:
在使用Spark Thrift Server对外提供服务的场景下，由于STS（Spark Thrift Server）固有的问题（例如服务与计算紧耦合，单租户，无高可用等），会影响到其提供服务的稳定性。
 
在这种需求场景下，可以考虑开源的Kyuubi解决方案。此框架除了解决STS固有的问题外，还从Spark 3.1. 开始，提供了一些企业级扩展插件，例如：

* 增强了AQE
* 提供自动小文件合并功能
* 支持限制查询的最大分区扫描量，从而限制查询结果大小
* 支持开箱即用的Z-Order优化
* 支持计算、写入Stage的配置隔离
* 增加skew join的处理能力
* Stage级别的config isolation

Question: EMRFS线程池问题
Answer: https://aws.amazon.com/cn/premiumsupport/knowledge-center/emr-timeout-connection-wait/

Question: EMR默认资源分配
Answer: 
EMR会根据启动的节点类型，自动配置Hadoop进程相关参数。推出这个功能的原因，是因为对于不同节点类型使用同一套配置，会造成资源浪费。所以需要对不同实例类型使用不同的资源配置，例如YARN_RESOURCEMANAGER_HEAPSIZE      ，YARN_PROXYSERVER_HEAPSIZE    ，YARN_NODEMANAGER_HEAPSIZE    ，HADOOP_JOB_HISTORYSERVER_HEAPSIZE，HADOOP_NAMENODE_HEAPSIZE        ，HADOOP_DATANODE_HEAPSIZE      
 
不同实例的具体配置可以参考：
https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-hadoop-daemons.html
 
Question: 扩缩容时注意事项
Answer: 
* 建议将HBase，Streaming以及Presto/Trino单独拆出作为独立集群。原因包括：
    * HBase的RegionServer会占据额外的内存，使得yarn nodemanager提供的计算内存更少
    * 在基于Yarn的指标进行弹性扩缩容时，HBase会受到影响。例如，假设HBase使用的是HBase on HDFS模式，则扩展/缩减 Task/Core实例组，也会导致HBase 的RS进程扩展与缩减（如果HBase使用的是HBase on S3模式，则Task节点上不会有RS进程，仅Core节点上有RS进程）
    * Streaming集群，特别是Flink，占用资源较为稳定
    * Presto/Trino不支持基于Yarn的自动扩展/缩减
    * Presto/Trino在首次集群启动后，其query memeroy的最高内存便已确定，手动加减节点不会修改此参数。需要通过EMR 的reconfiguation功能再做调整

* 实例组创建后，实例组的磁盘大小配置便不可修改（但仍可以单独扩展单个实例组的EBS卷大小）。此时若由于实例组的磁盘大小不够（例如spark shuffle 占据过多磁盘），会考虑创建新的实例组，并给予更大的磁盘空间。但是，若用了managed scaling，则仍可能会扩容原有实例组。

参考文档
https://aws.github.io/aws-emr-best-practices/
